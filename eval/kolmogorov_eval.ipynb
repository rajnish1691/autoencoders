{"cells":[{"cell_type":"markdown","metadata":{"id":"ZJj8fj8KziJ4"},"source":["#### Kolmogorov Data Evaluation"]},{"cell_type":"code","source":["# Connect Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/MyDrive/Colab Notebooks/Colab Notebooks/autoencoders\n","\n","%pwd"],"metadata":{"id":"Xz0FNqTjKgCg","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1765784697315,"user_tz":360,"elapsed":30283,"user":{"displayName":"Swati Shahi","userId":"11305998299635410874"}},"outputId":"e9914e07-f902-4cd7-cf3b-f742c0157c7c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/.shortcut-targets-by-id/1UMow24kXYpDLYgShcir7-CB3ZYQsgEih/Colab Notebooks/autoencoders\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/gdrive/.shortcut-targets-by-id/1UMow24kXYpDLYgShcir7-CB3ZYQsgEih/Colab Notebooks/autoencoders'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["# Encoded test pred.\n","import os\n","from pathlib import Path\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# PATHS\n","kolm_root        = Path(\"./datasets/kolmogorov_samples\")\n","kolm_out_root    = Path(\"./best_models/output_kolmogorov\")\n","\n","# input arrays\n","X_TEST_PATH = kolm_root / \"X_test.npy\"\n","\n","# output latents\n","enc_save_root = kolm_root / \"encoded_test_latent\"\n","enc_save_root.mkdir(parents=True, exist_ok=True)\n","\n","kolm_model_cfgs = {\n","    \"AuE\": {\n","        \"dir\":  kolm_out_root / \"output_kolm_ae\",\n","        \"ckpt\": \"best_overall_AuE.pt\",\n","    },\n","    \"VAE\": {\n","        \"dir\":  kolm_out_root / \"output_kolm_vae\",\n","        \"ckpt\": \"best_overall_VAE.pt\",\n","    },\n","    \"VQVAE\": {\n","        \"dir\":  kolm_out_root / \"output_kolm_vqvae\",\n","        \"ckpt\": \"best_overall_VQVAE.pt\",\n","    },\n","    \"VQVA2\": {\n","        \"dir\":  kolm_out_root / \"output_kolm_vqvae2\",\n","        \"ckpt\": \"best_overall_VQVA2.pt\",\n","    },\n","}\n","\n","BOTTLENECK_CH = 56\n","CODEBOOK_SIZE = 512\n","COMMIT_BETA   = 0.25\n","TOP_CH        = 56\n","DATA_SCALE    = 8.0\n","\n","class KolmogorovTestDataset(Dataset):\n","    def __init__(self, x_test_path: Path):\n","        self.X = np.load(x_test_path, mmap_mode=\"r\")\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        x = self.X[idx]\n","        x = np.array(x, copy=True)\n","        if x.ndim == 2:\n","            x = x[None, :, :]       # (1,H,W)\n","        x = torch.from_numpy(x).float() / DATA_SCALE\n","        return x\n","\n","assert X_TEST_PATH.exists(), f\"Missing: {X_TEST_PATH}\"\n","test_dataset = KolmogorovTestDataset(X_TEST_PATH)\n","test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n","\n","print(\"X_test samples:\", len(test_dataset))\n","print(\"Example shape:\", tuple(test_dataset[0].shape), \"dtype:\", test_dataset[0].dtype)\n","\n","class Snake(nn.Module):\n","    def __init__(self, alpha=1.0):\n","        super().__init__()\n","        self.alpha = nn.Parameter(torch.tensor(alpha))\n","\n","    def forward(self, x):\n","        a = self.alpha.abs() + 1e-6\n","        return x + (1.0 / a) * torch.sin(a * x).pow(2)\n","\n","class EncoderBlock(nn.Module):\n","    def __init__(self, cin, cout):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(cin, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","            nn.Conv2d(cout, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, cin, cout):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(cin, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","            nn.Conv2d(cout, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# MODELS\n","class AuE(nn.Module):\n","    def __init__(self, ch=BOTTLENECK_CH):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 128 -> 64\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, ch, 4, stride=2, padding=1),  # 64 -> 32\n","            Snake(),\n","        )\n","        self.dec = nn.Sequential(\n","            DecoderBlock(ch, 128),\n","            nn.ConvTranspose2d(128, 64, 2, stride=2),   # 32 -> 64\n","            Snake(),\n","            DecoderBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2),    # 64 -> 128\n","            Snake(),\n","            nn.Conv2d(32, 1, 1),\n","            nn.Tanh(),\n","        )\n","    def forward(self, x):\n","        z = self.enc(x)\n","        xhat = self.dec(z)\n","        return xhat, {}\n","\n","class VAE(nn.Module):\n","    def __init__(self, ch=BOTTLENECK_CH):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n","            Snake(),\n","        )\n","        self.mu     = nn.Conv2d(128, ch, 1)\n","        self.logvar = nn.Conv2d(128, ch, 1)\n","\n","        self.dec = nn.Sequential(\n","            DecoderBlock(ch, 128),\n","            nn.ConvTranspose2d(128, 64, 2, stride=2),\n","            Snake(),\n","            DecoderBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2),\n","            Snake(),\n","            nn.Conv2d(32, 1, 1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        h = self.enc(x)\n","        mu, logvar = self.mu(h), self.logvar(h)\n","        z = mu\n","        xhat = self.dec(z)\n","        return xhat, {\"mu\": mu, \"logvar\": logvar}\n","\n","class VectorQuantizer(nn.Module):\n","    def __init__(self, K, D, beta_commit=COMMIT_BETA):\n","        super().__init__()\n","        self.K = K\n","        self.D = D\n","        self.beta = beta_commit\n","        self.codebook = nn.Embedding(K, D)\n","        nn.init.uniform_(self.codebook.weight, -1.0 / D, 1.0 / D)\n","\n","    def forward(self, z_e):\n","        B, D, H, W = z_e.shape\n","        z = z_e.permute(0, 2, 3, 1).contiguous().view(-1, D)\n","        e = self.codebook.weight\n","        dist = (z.pow(2).sum(1, keepdim=True) + e.pow(2).sum(1) - 2 * z @ e.t())\n","        idx = torch.argmin(dist, dim=1)\n","        z_q = self.codebook(idx).view(B, H, W, D).permute(0, 3, 1, 2).contiguous()\n","        z_q_st = z_e + (z_q - z_e).detach()\n","        return z_q_st\n","\n","class VQVAE(nn.Module):\n","    def __init__(self, K=CODEBOOK_SIZE, D=BOTTLENECK_CH, beta_commit=COMMIT_BETA):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, D, 4, stride=2, padding=1),\n","            Snake(),\n","        )\n","        self.quant = VectorQuantizer(K, D, beta_commit=beta_commit)\n","\n","    def forward(self, x):\n","        z_e = self.encoder(x)\n","        z_q = self.quant(z_e)\n","        return z_q, {}\n","\n","class VQVA2(nn.Module):\n","    def __init__(self, K=CODEBOOK_SIZE, D=BOTTLENECK_CH, beta_commit=COMMIT_BETA, top_ch=TOP_CH):\n","        super().__init__()\n","        self.enc_bottom = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, D, 4, stride=2, padding=1),\n","            Snake(),\n","        )\n","        self.enc_top = nn.Sequential(\n","            nn.Conv2d(D, 128, 3, padding=1),\n","            Snake(),\n","            nn.Conv2d(128, top_ch, 4, stride=2, padding=1),  # 32 -> 16\n","            Snake(),\n","        )\n","\n","    def forward(self, x):\n","        zb = self.enc_bottom(x)          # (D,32,32)\n","        zt = self.enc_top(zb)            # (top_ch,16,16)\n","        zt_up = F.interpolate(zt, size=zb.shape[-2:], mode=\"nearest\")  # (top_ch,32,32)\n","        z_cat = torch.cat([zb, zt_up], dim=1)  # (D+top_ch,32,32)\n","        return z_cat, {}\n","\n","# LOAD BEST MODEL + ENCODE BATCH\n","def build_model_by_name(name: str) -> nn.Module:\n","    if name == \"AuE\":   return AuE()\n","    if name == \"VAE\":   return VAE()\n","    if name == \"VQVAE\": return VQVAE()\n","    if name == \"VQVA2\": return VQVA2()\n","    raise ValueError(f\"Unknown model: {name}\")\n","\n","def load_best_kolm_model_for_encoding(model_name: str) -> nn.Module:\n","    info = kolm_model_cfgs[model_name]\n","    ckpt_path = info[\"dir\"] / info[\"ckpt\"]\n","    assert ckpt_path.exists(), f\"Checkpoint not found: {ckpt_path}\"\n","\n","    state = torch.load(ckpt_path, map_location=\"cpu\")\n","\n","    if isinstance(state, dict) and \"model\" in state:\n","        sd = state[\"model\"]\n","    elif isinstance(state, dict) and \"model_state\" in state:\n","        sd = state[\"model_state\"]\n","    else:\n","        sd = state\n","\n","    model = build_model_by_name(model_name)\n","    model.load_state_dict(sd, strict=False)\n","    model.to(device).eval()\n","    return model\n","\n","@torch.no_grad()\n","def encode_batch(model_name: str, model: nn.Module, xb: torch.Tensor) -> torch.Tensor:\n","    xb = xb.to(device)\n","    if model_name == \"AuE\":\n","        return model.enc(xb).detach().cpu()\n","\n","    if model_name == \"VAE\":\n","        h = model.enc(xb)\n","        mu = model.mu(h)\n","        return mu.detach().cpu()\n","\n","    if model_name == \"VQVAE\":\n","        z_e = model.encoder(xb)\n","        return z_e.detach().cpu()\n","\n","    if model_name == \"VQVA2\":\n","        zb = model.enc_bottom(xb)\n","        zt = model.enc_top(zb)\n","        zt_up = F.interpolate(zt, size=zb.shape[-2:], mode=\"nearest\")\n","        return torch.cat([zb, zt_up], dim=1).detach().cpu()\n","\n","    raise ValueError(f\"Unknown model: {model_name}\")\n","\n","# ENCODE FULL TEST SET + SAVE\n","all_latent_paths = {}\n","\n","for name in [\"AuE\", \"VAE\", \"VQVAE\", \"VQVA2\"]:\n","    if name not in kolm_model_cfgs:\n","        continue\n","\n","    print(f\"\\n=== Encoding Kolmogorov test set – {name} ===\")\n","    model = load_best_kolm_model_for_encoding(name)\n","\n","    latents = []\n","    for xb in test_loader:\n","        z = encode_batch(name, model, xb)  # (B, C, 32, 32) or concat for VQVA2\n","        latents.append(z)\n","\n","    Z = torch.cat(latents, dim=0).numpy()\n","    out_path = enc_save_root / f\"kolm_{name}_test_latent.npy\"\n","    np.save(out_path, Z)\n","    all_latent_paths[name] = str(out_path)\n","\n","    print(f\"  Saved {name} latents: {Z.shape} -> {out_path}\")\n","\n","print(\"\\nAll encoded test latents saved:\")\n","for k, v in all_latent_paths.items():\n","    print(f\"  {k}: {v}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-CkPheE-Lm_","executionInfo":{"status":"ok","timestamp":1765785175634,"user_tz":360,"elapsed":11533,"user":{"displayName":"Swati Shahi","userId":"11305998299635410874"}},"outputId":"3cfe7c64-e90f-464e-aa5a-00f6d8c15765"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","X_test samples: 2000\n","Example shape: (1, 128, 128) dtype: torch.float32\n","\n","=== Encoding Kolmogorov test set – AuE ===\n","  Saved AuE latents: (2000, 56, 32, 32) -> datasets/kolmogorov_samples/encoded_test_latent/kolm_AuE_test_latent.npy\n","\n","=== Encoding Kolmogorov test set – VAE ===\n","  Saved VAE latents: (2000, 56, 32, 32) -> datasets/kolmogorov_samples/encoded_test_latent/kolm_VAE_test_latent.npy\n","\n","=== Encoding Kolmogorov test set – VQVAE ===\n","  Saved VQVAE latents: (2000, 56, 32, 32) -> datasets/kolmogorov_samples/encoded_test_latent/kolm_VQVAE_test_latent.npy\n","\n","=== Encoding Kolmogorov test set – VQVA2 ===\n","  Saved VQVA2 latents: (2000, 112, 32, 32) -> datasets/kolmogorov_samples/encoded_test_latent/kolm_VQVA2_test_latent.npy\n","\n","All encoded test latents saved:\n","  AuE: datasets/kolmogorov_samples/encoded_test_latent/kolm_AuE_test_latent.npy\n","  VAE: datasets/kolmogorov_samples/encoded_test_latent/kolm_VAE_test_latent.npy\n","  VQVAE: datasets/kolmogorov_samples/encoded_test_latent/kolm_VQVAE_test_latent.npy\n","  VQVA2: datasets/kolmogorov_samples/encoded_test_latent/kolm_VQVA2_test_latent.npy\n"]}]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","metadata":{"id":"ZJj8fj8KziJ4"},"source":["#### AutoEncoders Implementation\n","#### AE-VAE-VQVAE-VQVAE2\n","#### Dataset: MNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xz0FNqTjKgCg"},"outputs":[],"source":["# Connect Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/MyDrive/Colab Notebooks/Colab Notebooks/autoencoders\n","\n","%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPRzUl97Xhvt"},"outputs":[],"source":["import os, csv, json, time\n","from datetime import datetime\n","from pathlib import Path\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","# SOAP import (second-order optimizer)\n","try:\n","    from soap import SOAP\n","except Exception:\n","    SOAP = None\n","\n","\n","# ---------------- Config ----------------\n","class Cfg:\n","    ROOT      = Path(\".\")\n","    #ROOT      = Path(\"/home/krajnish/kolmogorov_data\")\n","    DATA_DIR  = ROOT / \"datasets/kolmogorov_samples\"\n","    OUT_DIR   = ROOT / \"best_models/output_kolmogorov/output_kolm_vaetest\"\n","\n","    # Training\n","    EPOCHS       = 1     #300\n","    BATCH_SIZE   = 64\n","    LR           = 1e-3\n","    WEIGHT_DECAY = 1e-4\n","    NUM_WORKERS  = 4\n","    PATIENCE     = None\n","    MIN_DELTA    = 1e-5\n","    GRAD_CLIP    = 1.0\n","\n","    # Optimizer/Precision\n","    PRECISION    = \"fp32\"          # \"fp32\" or \"fp64\"\n","    OPTIMIZER    = \"adam\"          # \"adam\" or \"soap\"\n","    ADAM_BETAS   = (0.9, 0.999)\n","\n","    # SOAP defaults\n","    SOAP_LR      = 3e-3\n","    SOAP_BETAS   = (0.95, 0.95)\n","    SOAP_WD      = 1e-2\n","    SOAP_PREFREQ = 10\n","\n","    # Shared latent/channel sizes\n","    BOTTLENECK_CH = 56             # latent channels\n","    LATENT_SHAPE  = (56, 32, 32)\n","\n","    # VQ specifics\n","    CODEBOOK_SIZE    = 512\n","    COMMIT_BETA      = 0.25\n","    VQ_WEIGHT        = 1.0\n","    VQ_TOP_WEIGHT    = 1.0\n","    VQ_BOTTOM_WEIGHT = 1.0\n","\n","    # VQ-VAE-2 specifics\n","    TOP_CH        = 56\n","\n","    # VAE KL weight\n","    VAE_BETA      = 1.0\n","\n","    # Scenarios\n","    #TRAIN_FRACTIONS_B    = [0.50, 0.60, 0.70]\n","    TRAIN_FRACTIONS_B    = [0.50, 0.60, 0.70]\n","    #TEST_NOISES          = [1, 5, 10]\n","    TEST_NOISES          = [1]\n","    TRAIN_FRACTION_FIXED = 0.70\n","\n","    # Logging / checkpoints\n","    CKPT_SUBDIR  = \"ckpts\"\n","    EXP_CSV      = OUT_DIR / \"experiments_all.csv\"\n","    LOSS_CSV     = OUT_DIR / \"loss_history_all.csv\"\n","    METRICS_CSV  = OUT_DIR / \"metrics_all.csv\"\n","\n","    SEED         = 42\n","    SHUFFLE      = False\n","    DATA_SCALE   = 8.0\n","\n","\n","cfg = Cfg()\n","cfg.OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# Device / seeds / dtype\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(cfg.SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(cfg.SEED)\n","torch.set_default_dtype(torch.float64 if cfg.PRECISION == \"fp64\" else torch.float32)\n","\n","print(\"[INFO] Device:\", device)\n","print(\"[INFO] Data dir:\", cfg.DATA_DIR.resolve())\n","\n","\n","# ---------------- Data loading ----------------\n","def add_noise(x: torch.Tensor, pct: float) -> torch.Tensor:\n","    if pct <= 0:\n","        return x\n","    std = float(pct) / 100.0\n","    return (x + torch.randn_like(x) * std).clamp(-1.0, 1.0)\n","\n","\n","\n","class KolmogorovDataset(Dataset):\n","\n","    def __init__(self, X: np.ndarray, noise_pct: float = 0.0):\n","        self.X = X\n","        self.noise_pct = float(noise_pct)\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        # Slice from mmap'ed array (likely read-only)\n","        x = self.X[idx]\n","\n","        # Make a writable copy before converting to tensor\n","        x = np.array(x, copy=True)          # or: x = x.copy()\n","\n","        x = torch.from_numpy(x).to(torch.float32)\n","        x = x / cfg.DATA_SCALE\n","        x_noisy = add_noise(x, self.noise_pct)\n","        return x_noisy, x\n","\n","\n","# Load full arrays (train/val/test)\n","X_TRAIN = np.load(cfg.DATA_DIR / \"X_train.npy\", mmap_mode=\"r\")\n","X_VAL   = np.load(cfg.DATA_DIR / \"X_val.npy\",   mmap_mode=\"r\")\n","X_TEST  = np.load(cfg.DATA_DIR / \"X_test.npy\",  mmap_mode=\"r\")\n","\n","print(\"[INFO] X_train.shape:\", X_TRAIN.shape, \"dtype:\", X_TRAIN.dtype)\n","print(\"[INFO] X_val.shape:  \", X_VAL.shape,   \"dtype:\", X_VAL.dtype)\n","print(\"[INFO] X_test.shape: \", X_TEST.shape,  \"dtype:\", X_TEST.dtype)\n","\n","\n","def build_loaders(train_fraction: float,\n","                  batch_size: int,\n","                  train_noise_pct: float = 0.0,\n","                  val_noise_pct: float   = 0.0,\n","                  test_noise_pct: float  = 0.0):\n","    n_train_full = X_TRAIN.shape[0]\n","    n_train_used = max(1, int(train_fraction * n_train_full))\n","\n","    Xtr_used = X_TRAIN[:n_train_used]\n","\n","    train_ds = KolmogorovDataset(Xtr_used, noise_pct=train_noise_pct)\n","    val_ds   = KolmogorovDataset(X_VAL,   noise_pct=val_noise_pct)\n","    test_ds  = KolmogorovDataset(X_TEST,  noise_pct=test_noise_pct)\n","\n","    pin = (device.type == \"cuda\")\n","    train_loader = DataLoader(\n","        train_ds, batch_size=batch_size, shuffle=cfg.SHUFFLE,\n","        num_workers=cfg.NUM_WORKERS, pin_memory=pin\n","    )\n","    val_loader = DataLoader(\n","        val_ds, batch_size=batch_size, shuffle=False,\n","        num_workers=cfg.NUM_WORKERS, pin_memory=pin\n","    )\n","    test_loader = DataLoader(\n","        test_ds, batch_size=batch_size, shuffle=False,\n","        num_workers=cfg.NUM_WORKERS, pin_memory=pin\n","    )\n","    return train_ds, val_ds, test_ds, train_loader, val_loader, test_loader\n","\n","\n","# ---------------- Snake activation ----------------\n","class Snake(nn.Module):\n","    \"\"\"\n","    Snake activation: x + (1/a) * sin^2(a x)\n","    \"\"\"\n","    def __init__(self, alpha=1.0):\n","        super().__init__()\n","        self.alpha = nn.Parameter(torch.tensor(alpha))\n","\n","    def forward(self, x):\n","        a = self.alpha.abs() + 1e-6\n","        return x + (1.0 / a) * torch.sin(a * x).pow(2)\n","\n","\n","# ---------------- Encoder/Decoder ----------------\n","class EncoderBlock(nn.Module):\n","    def __init__(self, cin, cout):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(cin, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","            nn.Conv2d(cout, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, cin, cout):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(cin, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","            nn.Conv2d(cout, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","# ---------------- Models ----------------\n","class AuE(nn.Module):\n","    \"\"\"Autoencoder with latent (ch, 32, 32) for 128x128 input.\"\"\"\n","    def __init__(self, ch=cfg.BOTTLENECK_CH):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 128 -> 64\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, ch, 4, stride=2, padding=1),  # 64 -> 32\n","            Snake(),\n","        )\n","        self.dec = nn.Sequential(\n","            DecoderBlock(ch, 128),\n","            nn.ConvTranspose2d(128, 64, 2, stride=2),   # 32 -> 64\n","            Snake(),\n","            DecoderBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2),    # 64 -> 128\n","            Snake(),\n","            nn.Conv2d(32, 1, 1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        z = self.enc(x)\n","        xhat = self.dec(z)\n","        return xhat, {\"aux_loss\": torch.tensor(0.0, device=x.device, dtype=x.dtype)}\n","\n","\n","# ---------- VAE ----------\n","class VAE(nn.Module):\n","    \"\"\"VAE with latent (ch, 32, 32).\"\"\"\n","    def __init__(self, ch=cfg.BOTTLENECK_CH):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 128 -> 64\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 64 -> 32\n","            Snake(),\n","        )\n","        self.mu     = nn.Conv2d(128, ch, 1)\n","        self.logvar = nn.Conv2d(128, ch, 1)\n","        self.dec = nn.Sequential(\n","            DecoderBlock(ch, 128),\n","            nn.ConvTranspose2d(128, 64, 2, stride=2),   # 32 -> 64\n","            Snake(),\n","            DecoderBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2),    # 64 -> 128\n","            Snake(),\n","            nn.Conv2d(32, 1, 1),\n","            nn.Tanh(),\n","        )\n","\n","    def reparam(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def forward(self, x):\n","        h = self.enc(x)\n","        mu, logvar = self.mu(h), self.logvar(h)\n","\n","        # Sample during training\n","        if self.training:\n","            z = self.reparam(mu, logvar)\n","        else:\n","            z = mu\n","\n","        xhat = self.dec(z)\n","\n","        # KL map: (B, C, H, W)\n","        kl_map = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n","        kl_per_sample = kl_map.mean(dim=[1, 2, 3])\n","        kl = kl_per_sample.mean()\n","\n","        return xhat, {\"aux_loss\": kl, \"mu\": mu, \"logvar\": logvar}\n","\n","\n","class VectorQuantizer(nn.Module):\n","    def __init__(self, K, D, beta_commit=0.25):\n","        super().__init__()\n","        self.K = K\n","        self.D = D\n","        self.beta = beta_commit\n","        self.codebook = nn.Embedding(K, D)\n","        nn.init.uniform_(self.codebook.weight, -1.0 / D, 1.0 / D)\n","\n","    def forward(self, z_e):\n","        B, D, H, W = z_e.shape\n","        z = z_e.permute(0, 2, 3, 1).contiguous().view(-1, D)  # (BHW, D)\n","        e = self.codebook.weight\n","        dist = (z.pow(2).sum(1, keepdim=True) + e.pow(2).sum(1) - 2 * z @ e.t())\n","        idx = torch.argmin(dist, dim=1)\n","        z_q = self.codebook(idx).view(B, H, W, D).permute(0, 3, 1, 2).contiguous()\n","\n","        codebook_loss = F.mse_loss(z_q.detach(), z_e)\n","        commit_loss   = self.beta * F.mse_loss(z_q, z_e.detach())\n","        vq_loss = codebook_loss + commit_loss\n","\n","        z_q_st = z_e + (z_q - z_e).detach()\n","\n","        with torch.no_grad():\n","            one_hot = F.one_hot(idx, num_classes=self.K).float()\n","            avg_probs = one_hot.mean(0)\n","            perplexity = torch.exp(-(avg_probs * (avg_probs + 1e-10).log()).sum())\n","        return z_q_st, vq_loss, perplexity, idx.view(B, H, W)\n","\n","\n","class VQVAE(nn.Module):\n","    def __init__(self, K=cfg.CODEBOOK_SIZE, D=cfg.BOTTLENECK_CH, beta_commit=cfg.COMMIT_BETA):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 128 -> 64\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, D, 4, stride=2, padding=1),   # 64 -> 32\n","            Snake(),\n","        )\n","        self.quant = VectorQuantizer(K, D, beta_commit=beta_commit)\n","        self.decoder = nn.Sequential(\n","            DecoderBlock(D, 128),\n","            nn.ConvTranspose2d(128, 64, 2, stride=2),   # 32 -> 64\n","            Snake(),\n","            DecoderBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2),    # 64 -> 128\n","            Snake(),\n","            nn.Conv2d(32, 1, 1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        z_e = self.encoder(x)\n","        z_q, vq_loss, ppl, idx = self.quant(z_e)\n","        xhat = self.decoder(z_q)\n","        aux = {\n","            \"aux_loss\": vq_loss,\n","            \"vq_loss\": vq_loss,\n","            \"perplexity\": ppl,\n","            \"indices\": idx,\n","        }\n","        return xhat, aux\n","\n","\n","class VQVA2(nn.Module):\n","    \"\"\"Two-level VQ-VAE-2.\"\"\"\n","    def __init__(self, K=cfg.CODEBOOK_SIZE, D=cfg.BOTTLENECK_CH,\n","                 beta_commit=cfg.COMMIT_BETA, top_ch=cfg.TOP_CH):\n","        super().__init__()\n","        self.enc_bottom = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),   # 128 -> 64\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, D, 4, stride=2, padding=1),    # 64 -> 32\n","            Snake(),\n","        )\n","        self.enc_top = nn.Sequential(\n","            nn.Conv2d(D, 128, 3, padding=1),\n","            Snake(),\n","            nn.Conv2d(128, top_ch, 4, stride=2, padding=1),  # 32 -> 16\n","            Snake(),\n","        )\n","        self.quant_top = VectorQuantizer(K, top_ch, beta_commit=beta_commit)\n","        self.bottom_condition = nn.Sequential(\n","            nn.Conv2d(D + top_ch, D, 1),\n","            Snake()\n","        )\n","        self.quant_bottom = VectorQuantizer(K, D, beta_commit=beta_commit)\n","        self.dec = nn.Sequential(\n","            DecoderBlock(D + top_ch, 128),\n","            nn.ConvTranspose2d(128, 64, 2, stride=2),   # 32\n","            Snake(),\n","            DecoderBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2),    # 64 -> 128\n","            Snake(),\n","            nn.Conv2d(32, 1, 1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        zb_e = self.enc_bottom(x)                       # (D,32,32)\n","        zt_e = self.enc_top(zb_e)                       # (top_ch,16,16)\n","        zt_q, vq_t, ppl_t, idx_t = self.quant_top(zt_e)\n","        zt_up = F.interpolate(zt_q, size=zb_e.shape[-2:], mode=\"nearest\")  # (top_ch,32,32)\n","        zb_cond = self.bottom_condition(torch.cat([zb_e, zt_up], dim=1))   # (D,32,32)\n","        zb_q, vq_b, ppl_b, idx_b = self.quant_bottom(zb_cond)\n","        dec_in = torch.cat([zb_q, zt_up], dim=1)        # (D+top_ch,32,32)\n","        xhat = self.dec(dec_in)\n","        aux = {\n","            \"aux_loss\": cfg.VQ_TOP_WEIGHT * vq_t + cfg.VQ_BOTTOM_WEIGHT * vq_b,\n","            \"vq_top\": vq_t,\n","            \"vq_bottom\": vq_b,\n","            \"perplexity_top\": ppl_t,\n","            \"perplexity_bottom\": ppl_b,\n","            \"indices_top\": idx_t,\n","            \"indices_bottom\": idx_b\n","        }\n","        return xhat, aux\n","\n","\n","# ---------------- Losses & metrics ----------------\n","def huber_recon(xhat, y, delta=1.0):\n","    diff = xhat - y\n","    absd = diff.abs()\n","    quad = torch.clamp(absd, max=delta)\n","    lin  = absd - quad\n","    return (0.5 * quad * quad / delta + lin).mean()\n","\n","\n","def rel_errors(xhat, y, eps=1e-12):\n","    diff = xhat - y\n","    relL1 = diff.abs().flatten(1).sum(1) / (y.abs().flatten(1).sum(1) + eps)\n","    relL2 = torch.sqrt((diff**2).flatten(1).sum(1)) / (torch.sqrt((y**2).flatten(1).sum(1) + eps))\n","    return relL1, relL2\n","\n","\n","# ---------------- Logging utilities ----------------\n","def _ensure_header(path: Path, fieldnames):\n","    write_header = not path.exists()\n","    f = path.open(\"a\", newline=\"\")\n","    w = csv.DictWriter(f, fieldnames=fieldnames)\n","    if write_header:\n","        w.writeheader()\n","    return f, w\n","\n","\n","def log_experiment_row(row: dict):\n","    fields = [\n","        \"timestamp\",\"run_name\",\"run_path\",\"model\",\"scenario\",\"sc\",\n","        \"epochs\",\"batch_size\",\"seed\",\n","        \"train_frac\",\"val_frac\",\"train_noise_pct\",\"val_noise_pct\",\"test_noise_pct\",\n","        \"bottleneck_ch\",\"codebook_size\",\"commitment_beta\",\"top_ch\",\n","        \"precision\",\"optimizer\",\n","        \"best_val\",\"best_ckpt\",\"last_ckpt\"\n","    ]\n","    f, w = _ensure_header(cfg.EXP_CSV, fields)\n","    w.writerow({k: row.get(k, None) for k in fields})\n","    f.close()\n","\n","\n","def log_loss_history_rows(run_name, run_path, model, scenario, epoch, train_total, val_total, ppl=None, extra=None):\n","    fields = [\"run_name\",\"run_path\",\"model\",\"scenario\",\"epoch\",\"train_total\",\"val_total\",\"ppl\",\"extra_json\"]\n","    f, w = _ensure_header(cfg.LOSS_CSV, fields)\n","    w.writerow({\n","        \"run_name\": run_name,\n","        \"run_path\": str(run_path),\n","        \"model\": model,\n","        \"scenario\": scenario,\n","        \"epoch\": epoch,\n","        \"train_total\": train_total,\n","        \"val_total\": val_total,\n","        \"ppl\": ppl if (ppl is not None) else \"\",\n","        \"extra_json\": json.dumps(extra or {})\n","    })\n","    f.close()\n","\n","\n","def log_metrics_row(row: dict):\n","    fields = [\n","        \"run_name\",\"run_path\",\"model\",\"scenario\",\"sc\",\n","        \"train_frac\",\"val_frac\",\"train_noise_pct\",\"val_noise_pct\",\"test_noise_pct\",\n","        \"recon_huber_mean\",\"aux_loss_mean\",\"total_loss_mean\",\n","        \"relL1_mean\",\"relL2_mean\",\n","        \"train_size\",\"val_size\",\"test_size\",\n","        \"precision\",\"optimizer\"\n","    ]\n","    f, w = _ensure_header(cfg.METRICS_CSV, fields)\n","    w.writerow({k: row.get(k, None) for k in fields})\n","    f.close()\n","\n","\n","# ---------------- Checkpoint manager ----------------\n","class CheckpointManager:\n","    def __init__(self, ckpt_dir: Path):\n","        self.ckpt_dir = Path(ckpt_dir)\n","        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n","        self.best_path = self.ckpt_dir / \"best.pt\"\n","        self.last_path = self.ckpt_dir / \"last.pt\"\n","\n","    def save(self, epoch, model, optimizer, scheduler, best_val, val_loss, history, extras=None):\n","        state = {\n","            \"epoch\": epoch,\n","            \"model\": model.state_dict(),\n","            \"optimizer\": optimizer.state_dict() if optimizer else None,\n","            \"scheduler\": scheduler.state_dict() if scheduler else None,\n","            \"best_val\": best_val,\n","            \"val_loss\": val_loss,\n","            \"history\": history,\n","            \"extras\": extras or {},\n","            \"seed\": cfg.SEED,\n","        }\n","        torch.save(state, self.last_path)\n","        if val_loss + 1e-12 < best_val - cfg.MIN_DELTA:\n","            torch.save(state, self.best_path)\n","            return True\n","        return False\n","\n","\n","# ---------------- Training helpers ----------------\n","def build_model(model_name: str):\n","    if model_name == \"AuE\":\n","        return AuE(ch=cfg.BOTTLENECK_CH)\n","    if model_name == \"VAE\":\n","        return VAE(ch=cfg.BOTTLENECK_CH)\n","    if model_name == \"VQVAE\":\n","        return VQVAE(K=cfg.CODEBOOK_SIZE, D=cfg.BOTTLENECK_CH, beta_commit=cfg.COMMIT_BETA)\n","    if model_name == \"VQVA2\":\n","        return VQVA2(K=cfg.CODEBOOK_SIZE, D=cfg.BOTTLENECK_CH, beta_commit=cfg.COMMIT_BETA, top_ch=cfg.TOP_CH)\n","    raise ValueError(f\"Unknown model: {model_name}\")\n","\n","\n","def forward_and_losses(model_name: str, model, xb, yb):\n","    xhat, info = model(xb)\n","    rec = huber_recon(xhat, yb)\n","    aux_term = info.get(\"aux_loss\", torch.tensor(0.0, device=xb.device, dtype=xb.dtype))\n","\n","    if model_name == \"VAE\":\n","        # recon + β * KL\n","        loss = rec + cfg.VAE_BETA * aux_term\n","        ppl = None\n","\n","    elif model_name == \"VQVAE\":\n","        vq_loss = info.get(\"vq_loss\", aux_term)\n","        loss = rec + cfg.VQ_WEIGHT * vq_loss\n","        ppl = float(info.get(\"perplexity\", float(\"nan\")))\n","\n","    elif model_name == \"VQVA2\":\n","        vq_t = info.get(\"vq_top\", None)\n","        vq_b = info.get(\"vq_bottom\", None)\n","        if (vq_t is None) or (vq_b is None):\n","            loss = rec + aux_term\n","        else:\n","            loss = rec + cfg.VQ_TOP_WEIGHT * vq_t + cfg.VQ_BOTTOM_WEIGHT * vq_b\n","        ppl_t = float(info.get(\"perplexity_top\", float(\"nan\")))\n","        ppl_b = float(info.get(\"perplexity_bottom\", float(\"nan\")))\n","        ppl = (ppl_t + ppl_b) / 2.0\n","\n","    else:  # AuE\n","        loss = rec\n","        ppl = None\n","\n","    return xhat, loss, rec, aux_term, ppl\n","\n","\n","def make_optimizer(model):\n","    opt_name = cfg.OPTIMIZER.lower()\n","    if opt_name == \"soap\":\n","        assert SOAP is not None, \"SOAP optimizer not found\"\n","        return SOAP(\n","            params=model.parameters(),\n","            lr=cfg.SOAP_LR,\n","            betas=cfg.SOAP_BETAS,\n","            weight_decay=cfg.SOAP_WD,\n","            precondition_frequency=cfg.SOAP_PREFREQ\n","        )\n","    return torch.optim.Adam(\n","        model.parameters(),\n","        lr=cfg.LR,\n","        betas=cfg.ADAM_BETAS,\n","        weight_decay=cfg.WEIGHT_DECAY\n","    )\n","\n","\n","def train_eval_once(model_name: str,\n","                    train_frac: float,\n","                    train_noise_pct: float,\n","                    val_noise_pct: float,\n","                    test_noise_pct: float,\n","                    tag: str):\n","\n","    run_root = cfg.OUT_DIR / f\"{model_name.lower()}_{tag}_{int(train_frac*100)}_tn{train_noise_pct}_vn{val_noise_pct}_ts{test_noise_pct}_{cfg.PRECISION}_{cfg.OPTIMIZER}_{time.strftime('%Y%m%d_%H%M%S')}\"\n","    run_root.mkdir(parents=True, exist_ok=True)\n","    ckpt_dir = run_root / cfg.CKPT_SUBDIR\n","    cpm = CheckpointManager(ckpt_dir)\n","\n","    train_ds, val_ds, test_ds, train_loader, val_loader, test_loader = build_loaders(\n","        train_fraction=train_frac, batch_size=cfg.BATCH_SIZE,\n","        train_noise_pct=train_noise_pct, val_noise_pct=val_noise_pct, test_noise_pct=test_noise_pct\n","    )\n","\n","    model = build_model(model_name).to(device)\n","    if cfg.PRECISION == \"fp64\":\n","        model = model.double()\n","\n","    opt = make_optimizer(model)\n","    sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=max(5, cfg.EPOCHS))\n","\n","    best_val, no_imp = float(\"inf\"), 0\n","    history = {\"train_total\": [], \"val_total\": [], \"ppl\": []}\n","\n","    for epoch in range(1, cfg.EPOCHS + 1):\n","        # Train\n","        model.train()\n","        t_losses, t_ppls = [], []\n","        for xb, yb in train_loader:\n","            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n","            if cfg.PRECISION == \"fp64\":\n","                xb = xb.to(torch.float64)\n","                yb = yb.to(torch.float64)\n","\n","            opt.zero_grad(set_to_none=True)\n","            _, loss, rec, aux, ppl = forward_and_losses(model_name, model, xb, yb)\n","            loss.backward()\n","            if cfg.GRAD_CLIP:\n","                nn.utils.clip_grad_norm_(model.parameters(), cfg.GRAD_CLIP)\n","            opt.step()\n","\n","            t_losses.append(loss.item())\n","            if ppl is not None:\n","                t_ppls.append(ppl)\n","        sch.step(epoch)\n","\n","        # Val\n","        model.eval()\n","        v_losses, v_ppls = [], []\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                xb, yb = xb.to(device), yb.to(device)\n","                if cfg.PRECISION == \"fp64\":\n","                    xb = xb.to(torch.float64)\n","                    yb = yb.to(torch.float64)\n","                _, loss, rec, aux, ppl = forward_and_losses(model_name, model, xb, yb)\n","                v_losses.append(loss.item())\n","                if ppl is not None:\n","                    v_ppls.append(ppl)\n","\n","        v_mean = float(np.mean(v_losses)) if v_losses else float(\"inf\")\n","        ppl_mean = float(np.mean(v_ppls)) if v_ppls else float(\"nan\")\n","        history[\"train_total\"].append(float(np.mean(t_losses)) if t_losses else float(\"inf\"))\n","        history[\"val_total\"].append(v_mean)\n","        history[\"ppl\"].append(ppl_mean)\n","\n","        log_loss_history_rows(\n","            run_name=run_root.name, run_path=str(run_root), model=model_name, scenario=tag,\n","            epoch=epoch, train_total=history[\"train_total\"][-1], val_total=v_mean, ppl=ppl_mean\n","        )\n","\n","        improved = (best_val - v_mean) > cfg.MIN_DELTA\n","        if improved:\n","            best_val, no_imp = v_mean, 0\n","        else:\n","            no_imp += 1\n","\n","        cpm.save(epoch=epoch, model=model, optimizer=opt, scheduler=sch,\n","                 best_val=best_val, val_loss=v_mean, history=history,\n","                 extras={\"scenario\": tag, \"train_frac\": train_frac})\n","\n","        print(f\"[{model_name}/{tag}/{cfg.PRECISION}/{cfg.OPTIMIZER}] Epoch {epoch:03d} | val={v_mean:.6f} | ppl={ppl_mean if not np.isnan(ppl_mean) else '—'}\")\n","\n","        if (cfg.PATIENCE is not None) and (no_imp >= cfg.PATIENCE):\n","            print(\"Early stopping.\")\n","            break\n","\n","    # Save local history\n","    with open(run_root / \"history.json\", \"w\") as f:\n","        json.dump(history, f, indent=2)\n","    with open(run_root / \"loss_history.csv\", \"w\", newline=\"\") as f:\n","        w = csv.writer(f)\n","        w.writerow([\"epoch\",\"train_total\",\"val_total\",\"ppl\"])\n","        for e,(tr,va,pp) in enumerate(zip(history[\"train_total\"], history[\"val_total\"], history[\"ppl\"]), start=1):\n","            w.writerow([e, tr, va, pp])\n","\n","    # Test with best ckpt\n","    best_to_load = (ckpt_dir / \"best.pt\") if (ckpt_dir / \"best.pt\").exists() else (ckpt_dir / \"last.pt\")\n","    print(\"Testing with:\", best_to_load)\n","    state = torch.load(best_to_load, map_location=device)\n","    model.load_state_dict(state[\"model\"])\n","    model.eval()\n","\n","    recon_vals, aux_vals, total_vals, r1_vals, r2_vals = [], [], [], [], []\n","    with torch.no_grad():\n","        for xb, yb in test_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            if cfg.PRECISION == \"fp64\":\n","                xb = xb.to(torch.float64)\n","                yb = yb.to(torch.float64)\n","            xhat, loss, rec, aux, ppl = forward_and_losses(model_name, model, xb, yb)\n","            recon_vals.append(rec.item())\n","            aux_vals.append(float(aux))\n","            total_vals.append(loss.item())\n","            r1, r2 = rel_errors(xhat, yb)\n","            r1_vals.append(r1.cpu())\n","            r2_vals.append(r2.cpu())\n","\n","    recon_mean = float(np.mean(recon_vals))\n","    aux_mean   = float(np.mean(aux_vals)) if aux_vals else 0.0\n","    total_mean = float(np.mean(total_vals))\n","    relL1_mean = torch.cat(r1_vals).mean().item()\n","    relL2_mean = torch.cat(r2_vals).mean().item()\n","\n","    # Save per-sample metrics\n","    r1_all = torch.cat(r1_vals).numpy()\n","    r2_all = torch.cat(r2_vals).numpy()\n","    np.savez_compressed(run_root / \"per_sample_metrics.npz\",\n","                        relL1=r1_all,\n","                        relL2=r2_all,\n","                        indices=np.arange(len(r1_all)))\n","\n","    print(\n","        f\"[TEST {model_name}/{tag}/{cfg.PRECISION}/{cfg.OPTIMIZER}] \"\n","        f\"recon={recon_mean:.6f} aux={aux_mean:.6f} total={total_mean:.6f} \"\n","        f\"relL1={relL1_mean:.6f} relL2={relL2_mean:.6f}\"\n","    )\n","\n","    info_row = {\n","        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n","        \"run_name\": run_root.name,\n","        \"run_path\": str(run_root.resolve()),\n","        \"model\": model_name,\n","        \"scenario\": tag,\n","        \"sc\": tag[:1] if tag else tag,\n","        \"epochs\": cfg.EPOCHS,\n","        \"batch_size\": cfg.BATCH_SIZE,\n","        \"seed\": cfg.SEED,\n","        \"train_frac\": train_frac,\n","        \"val_frac\": 1.0 - train_frac,\n","        \"train_noise_pct\": train_noise_pct,\n","        \"val_noise_pct\": val_noise_pct,\n","        \"test_noise_pct\": test_noise_pct,\n","        \"bottleneck_ch\": cfg.BOTTLENECK_CH,\n","        \"codebook_size\": cfg.CODEBOOK_SIZE,\n","        \"commitment_beta\": cfg.COMMIT_BETA,\n","        \"top_ch\": cfg.TOP_CH if model_name == \"VQVA2\" else \"\",\n","        \"precision\": cfg.PRECISION,\n","        \"optimizer\": cfg.OPTIMIZER,\n","        \"best_val\": best_val,\n","        \"best_ckpt\": str((ckpt_dir / \"best.pt\").resolve()) if (ckpt_dir / \"best.pt\").exists() else \"\",\n","        \"last_ckpt\": str((ckpt_dir / \"last.pt\").resolve()) if (ckpt_dir / \"last.pt\").exists() else \"\",\n","    }\n","    log_experiment_row(info_row)\n","\n","    metrics_row = {\n","        \"run_name\": run_root.name, \"run_path\": str(run_root.resolve()),\n","        \"model\": model_name, \"scenario\": tag, \"sc\": tag[:1] if tag else tag,\n","        \"train_frac\": train_frac, \"val_frac\": 1.0 - train_frac,\n","        \"train_noise_pct\": train_noise_pct, \"val_noise_pct\": val_noise_pct, \"test_noise_pct\": test_noise_pct,\n","        \"recon_huber_mean\": recon_mean, \"aux_loss_mean\": aux_mean, \"total_loss_mean\": total_mean,\n","        \"relL1_mean\": relL1_mean, \"relL2_mean\": relL2_mean,\n","        \"train_size\": len(train_ds), \"val_size\": len(val_ds), \"test_size\": len(test_ds),\n","        \"precision\": cfg.PRECISION, \"optimizer\": cfg.OPTIMIZER\n","    }\n","    log_metrics_row(metrics_row)\n","\n","    print(f\"[DONE {model_name}/{tag}/{cfg.PRECISION}/{cfg.OPTIMIZER}] run_dir → {run_root.resolve()}\")\n","    return run_root, info_row, metrics_row\n","\n","\n","# ---------------- Scenario runner ----------------\n","def run_scenarios_for_model(model_name: str):\n","    print(f\"\\n==================== {model_name} ({cfg.PRECISION}/{cfg.OPTIMIZER}) ====================\")\n","    results = []\n","    # b) split sweep, clean test\n","    for tf in cfg.TRAIN_FRACTIONS_B:\n","        results.append(train_eval_once(model_name, tf, 0.0, 0.0, 0.0, tag=\"b_split\"))\n","    # c) fixed split, test noise sweep\n","    for tn in cfg.TEST_NOISES:\n","        results.append(train_eval_once(model_name, cfg.TRAIN_FRACTION_FIXED, 0.0, 0.0, float(tn), tag=\"c_test_noise\"))\n","    # d) fixed split, noise on train + same noise on test\n","    for tn in cfg.TEST_NOISES:\n","        results.append(train_eval_once(model_name, cfg.TRAIN_FRACTION_FIXED, float(tn), 0.0, float(tn), tag=\"d_train_and_test_noise\"))\n","    return results\n","\n","\n","# ---------------- Main ----------------\n","if __name__ == \"__main__\":\n","    # ALL_MODELS = [\"AuE\", \"VAE\", \"VQVAE\", \"VQVA2\"]\n","    ALL_MODELS = [\"VAE\"]\n","\n","    all_results_by_model = {m: [] for m in ALL_MODELS}\n","\n","    for prec in [\"fp32\", \"fp64\"]:\n","        for opt in [\"adam\", \"soap\"]:\n","            cfg.PRECISION = prec\n","            cfg.OPTIMIZER = opt\n","            if cfg.OPTIMIZER == \"soap\" and SOAP is None:\n","                raise RuntimeError(\"Requested SOAP but soap.py not found\")\n","            torch.set_default_dtype(torch.float64 if cfg.PRECISION == \"fp64\" else torch.float32)\n","\n","            for m in ALL_MODELS:\n","                results = run_scenarios_for_model(m)\n","                all_results_by_model[m].extend(results)\n","\n","    # pick the best run per model\n","    for m in ALL_MODELS:\n","        runs = all_results_by_model[m]\n","        if not runs:\n","            continue\n","        best_run = min(runs, key=lambda r: r[2][\"total_loss_mean\"])\n","        best_run_root, best_info, best_metrics = best_run\n","        best_ckpt_path = best_info.get(\"best_ckpt\") or best_info.get(\"last_ckpt\")\n","        if best_ckpt_path:\n","            state = torch.load(best_ckpt_path, map_location=\"cpu\")\n","            best_out_path = cfg.OUT_DIR / f\"best_overall_{m}.pt\"\n","            torch.save(state, best_out_path)\n","            print(\n","                f\"\\n[GLOBAL BEST] {m} total_loss_mean={best_metrics['total_loss_mean']:.6f} \"\n","                f\"(run={best_info['run_name']}, prec={best_info['precision']}, opt={best_info['optimizer']})\\n\"\n","                f\"Saved to: {best_out_path}\"\n","            )\n","\n","    print(\"\\nAll experiments finished.\")\n","    print(f\"Registry: {cfg.EXP_CSV.resolve()}\")\n","    print(f\"Loss CSV: {cfg.LOSS_CSV.resolve()}\")\n","    print(f\"Metrics:  {cfg.METRICS_CSV.resolve()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrJQsa2-XiFu"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
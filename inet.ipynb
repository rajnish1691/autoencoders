{"cells":[{"cell_type":"markdown","metadata":{"id":"ZJj8fj8KziJ4"},"source":["#### AE-VAE-VQVAE-VQVAE2 Implementation\n","#### Dataset: Imagenet100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbQPlX0KvwRH"},"outputs":[],"source":["# Connect Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/MyDrive/Colab Notebooks/Colab Notebooks/autoencoders\n","\n","%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pWB-CP8t0_6T"},"outputs":[],"source":["import os, csv, json, time, shutil, tempfile\n","from datetime import datetime\n","from pathlib import Path\n","import numpy as np\n","import torch, torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# path\n","#OUT_DIR   = Path(\"/home/krajnish/autoencoders/best_models/output_inet/output_inet_vae\")\n","OUT_DIR   = Path(\"./best_models/output_inet/output_inet_vaetest\")\n","#TENSORCACHE_DIR   = Path(\"/home/krajnish/autoencoders\")\n","TENSORCACHE_DIR   = Path(\".\")\n","CACHE_DIR = TENSORCACHE_DIR / \"datasets/inet100\"\n","TRAIN_PT  = CACHE_DIR / \"train.pt\"\n","TEST_PT   = CACHE_DIR / \"test.pt\"\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# SOAP import (second-order optimizer)\n","try:\n","    from soap import SOAP\n","except Exception:\n","    SOAP = None\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","print(\"Using cached tensors:\")\n","print(\" -\", TRAIN_PT, TRAIN_PT.exists())\n","print(\" -\", TEST_PT,  TEST_PT.exists())\n","if not TRAIN_PT.exists() or not TEST_PT.exists():\n","    raise FileNotFoundError(\"cache not found\")\n","\n","# ---------------- Config ----------------\n","class CFG:\n","    # Training\n","    EPOCHS       = 1      #300\n","    BATCH_SIZE   = 256\n","    LR           = 1e-3\n","    WEIGHT_DECAY = 1e-4\n","    NUM_WORKERS  = 4\n","    PIN_MEMORY   = torch.cuda.is_available()\n","    PERSISTENT   = (NUM_WORKERS > 0)\n","    PATIENCE     = None\n","    MIN_DELTA    = 1e-5\n","    GRAD_CLIP    = 1.0\n","    SEED         = 42\n","\n","    # Precision / Optimizer\n","    PRECISION    = \"fp32\"          # Precision: \"fp32\", \"fp64\"\n","    OPTIMIZER    = \"adam\"          # Optimizer: \"adam\", \"soap\"\n","    ADAM_BETAS   = (0.9, 0.999)\n","\n","    # SOAP recommended defaults\n","    SOAP_LR      = 3e-3\n","    SOAP_BETAS   = (0.95, 0.95)\n","    SOAP_WD      = 1e-2\n","    SOAP_PREFREQ = 10\n","\n","    # Latent/channel sizes\n","    BOTTLENECK_CH = 56\n","    LATENT_SHAPE  = (56, 7, 7)\n","\n","    # VQ specifics\n","    CODEBOOK_SIZE = 512\n","    COMMIT_BETA   = 0.25\n","\n","    # VQ loss weights\n","    VQ_WEIGHT         = 1.0   # for VQVE (VQ-VAE)\n","    VQ_TOP_WEIGHT     = 1.0   # for VQVA2 top level\n","    VQ_BOTTOM_WEIGHT  = 1.0   # for VQVA2 bottom level\n","\n","    # VQ-VAE-2 specifics\n","    TOP_CH        = 56\n","\n","    # VAE KL weight\n","    VAE_BETA      = 1.0\n","\n","    # Scenarios\n","    #TRAIN_FRACTIONS_B = [0.50, 0.60, 0.70]     # scenario b\n","    TRAIN_FRACTIONS_B = [0.70]\n","    #TEST_NOISES       = [1, 5, 10]             # scenarios c & d\n","    TEST_NOISES       = [1]\n","    TRAIN_FRACTION_FIXED = 0.70                 # for c/d\n","\n","    # Central CSVs\n","    EXP_CSV  = OUT_DIR / \"experiments_inet100.csv\"\n","    LOSS_CSV = OUT_DIR / \"loss_history_inet100.csv\"\n","    MET_CSV  = OUT_DIR / \"metrics_inet100.csv\"\n","\n","cfg = CFG()\n","torch.manual_seed(cfg.SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(cfg.SEED)\n","\n","torch.set_default_dtype(torch.float64 if cfg.PRECISION == \"fp64\" else torch.float32)\n","\n","# ---------------- Snake activation ----------------\n","class Snake(nn.Module):\n","    \"\"\"\n","    Snake activation: x + (1/a) * sin^2(a x)\n","    \"\"\"\n","    def __init__(self, alpha=1.0):\n","        super().__init__()\n","        self.alpha = nn.Parameter(torch.tensor(alpha))\n","\n","    def forward(self, x):\n","        a = self.alpha.abs() + 1e-6\n","        return x + (1.0 / a) * torch.sin(a * x).pow(2)\n","\n","# ---------------- CSV helpers ----------------\n","def _read_header_if_exists(path: Path):\n","    if not path.exists() or path.stat().st_size == 0:\n","        return None\n","    with path.open(\"r\", newline=\"\") as f:\n","        reader = csv.reader(f)\n","        try:\n","            hdr = next(reader); return hdr\n","        except StopIteration:\n","            return None\n","\n","def _upgrade_csv_header_if_needed(path: Path, desired_fields):\n","    existing = _read_header_if_exists(path)\n","    if existing is None:\n","        return desired_fields[:]\n","    if set(desired_fields).issubset(existing):\n","        return existing[:]\n","\n","    new_fields = existing[:] + [f for f in desired_fields if f not in existing]\n","    tmp = Path(str(path) + \".tmp\")\n","    with path.open(\"r\", newline=\"\") as fin, tmp.open(\"w\", newline=\"\") as fout:\n","        rin = csv.DictReader(fin)\n","        wout = csv.DictWriter(fout, fieldnames=new_fields)\n","        wout.writeheader()\n","        for row in rin:\n","            for k in new_fields:\n","                row.setdefault(k, \"\")\n","            wout.writerow({k: row.get(k, \"\") for k in new_fields})\n","    shutil.move(tmp, path)\n","    return new_fields\n","\n","def _ensure_writer(path: Path, desired_fields):\n","    path.parent.mkdir(parents=True, exist_ok=True)\n","    final_fields = _upgrade_csv_header_if_needed(path, desired_fields)\n","    f = path.open(\"a\", newline=\"\")\n","    w = csv.DictWriter(f, fieldnames=final_fields)\n","    if path.stat().st_size == 0:\n","        w.writeheader()\n","    return f, w, final_fields\n","\n","def log_exp(row: dict):\n","    fields = [\n","        \"timestamp\",\"dataset\",\"run_name\",\"run_path\",\"model\",\"scenario\",\"sc\",\n","        \"epochs\",\"batch_size\",\"seed\",\n","        \"train_frac\",\"val_frac\",\"train_noise_pct\",\"val_noise_pct\",\"test_noise_pct\",\n","        \"bottleneck_ch\",\"best_val\",\n","        \"precision\",\"optimizer\"\n","    ]\n","    f, w, cols = _ensure_writer(cfg.EXP_CSV, fields)\n","    w.writerow({k: row.get(k, \"\") for k in cols}); f.close()\n","\n","def log_loss(run_info: dict, epoch: int, train_total: float, val_total: float):\n","    fields = [\"dataset\",\"run_name\",\"run_path\",\"model\",\"scenario\",\"epoch\",\"train_total\",\"val_total\"]\n","    f, w, cols = _ensure_writer(cfg.LOSS_CCSV, fields) if False else _ensure_writer(cfg.LOSS_CSV, fields)\n","    row = {\n","        \"dataset\": \"inet100\",\n","        \"run_name\": run_info.get(\"run_name\",\"\"),\n","        \"run_path\": run_info.get(\"run_path\",\"\"),\n","        \"model\":    run_info.get(\"model\",\"\"),\n","        \"scenario\": run_info.get(\"scenario\",\"\"),\n","        \"epoch\": epoch,\n","        \"train_total\": train_total,\n","        \"val_total\": val_total,\n","    }\n","    w.writerow({k: row.get(k, \"\") for k in cols}); f.close()\n","\n","def log_metrics(row: dict):\n","    fields = [\n","        \"dataset\",\"run_name\",\"run_path\",\"model\",\"scenario\",\"sc\",\n","        \"train_frac\",\"val_frac\",\"train_noise_pct\",\"val_noise_pct\",\"test_noise_pct\",\n","        \"recon_huber_mean\",\"aux_loss_mean\",\"total_loss_mean\",\n","        \"relL1_mean\",\"relL2_mean\",\n","        \"train_size\",\"val_size\",\"test_size\",\n","        \"precision\",\"optimizer\"\n","    ]\n","    f, w, cols = _ensure_writer(cfg.MET_CSV, fields)\n","    w.writerow({k: row.get(k, \"\") for k in cols}); f.close()\n","\n","# ---------------- Dataset / Loader ----------------\n","def _noisy_dataset_from_tensor(X: torch.Tensor, noise_pct: float):\n","    class PairDS(torch.utils.data.Dataset):\n","        def __init__(self, base, pct): self.base, self.std = base, noise_pct/100.0\n","        def __len__(self): return len(self.base)\n","        def __getitem__(self, i):\n","            x = self.base[i][0].float()  # cache typically fp32\n","            if self.std > 0:\n","                x_noisy = (x + torch.randn_like(x) * self.std).clamp(-1, 1)\n","            else:\n","                x_noisy = x.clone()\n","            return x_noisy, x\n","    return PairDS(TensorDataset(X), noise_pct)\n","\n","def build_loaders(train_fraction: float, batch_size: int,\n","                  train_noise_pct: float = 0.0,\n","                  val_noise_pct: float   = 0.0,\n","                  test_noise_pct: float  = 0.0):\n","    Xtr = torch.load(TRAIN_PT, map_location=\"cpu\", weights_only=True)\n","    Xte = torch.load(TEST_PT,  map_location=\"cpu\", weights_only=True)\n","    n_train = int(round(train_fraction * Xtr.shape[0]))\n","    X_train, X_val = Xtr[:n_train], Xtr[n_train:]\n","\n","    train_ds = _noisy_dataset_from_tensor(X_train, train_noise_pct)\n","    val_ds   = _noisy_dataset_from_tensor(X_val,   val_noise_pct)\n","    test_ds  = _noisy_dataset_from_tensor(Xte,     test_noise_pct)\n","\n","    kw = dict(batch_size=batch_size, shuffle=False,\n","              num_workers=cfg.NUM_WORKERS, pin_memory=cfg.PIN_MEMORY,\n","              persistent_workers=cfg.PERSISTENT)\n","    return (train_ds, val_ds, test_ds,\n","            DataLoader(train_ds, **kw),\n","            DataLoader(val_ds,   **kw),\n","            DataLoader(test_ds,  **kw))\n","\n","# ---------------- Losses / Metrics ----------------\n","def huber_recon(xhat, y, delta=1.0):\n","    d = xhat - y\n","    a = d.abs()\n","    q = torch.clamp(a, max=delta)\n","    l = a - q\n","    return (0.5 * q * q / delta + l).mean()\n","\n","def rel_errors(xhat, y, eps=1e-12):\n","    d = xhat - y\n","    r1 = d.abs().flatten(1).sum(1) / (y.abs().flatten(1).sum(1) + eps)\n","    r2 = torch.sqrt((d**2).flatten(1).sum(1)) / (torch.sqrt((y**2).flatten(1).sum(1) + eps))\n","    return r1, r2\n","\n","# ---------------- Models ----------------\n","class AuE(nn.Module):\n","    def __init__(self, ch=cfg.BOTTLENECK_CH):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            nn.Conv2d(1, 32, 3, padding=1), Snake(),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1), Snake(),\n","            nn.Conv2d(64, ch, 4, stride=2, padding=1), Snake()   # -> 56x7x7 + Snake\n","        )\n","        self.dec = nn.Sequential(\n","            nn.ConvTranspose2d(ch, 64, 2, stride=2), Snake(),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2), Snake(),\n","            nn.Conv2d(32, 1, 1), nn.Tanh()\n","        )\n","    def forward(self, x):\n","        z = self.enc(x)\n","        xh = self.dec(z)\n","        return xh, {}\n","\n","# ---------- VAE ----------\n","class VAE(nn.Module):\n","    def __init__(self, ch=cfg.BOTTLENECK_CH):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            nn.Conv2d(1, 32, 3, padding=1), Snake(),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1), Snake()\n","        )\n","        self.mu     = nn.Conv2d(64, ch, 3, padding=1)\n","        self.logvar = nn.Conv2d(64, ch, 3, padding=1)\n","        self.dec = nn.Sequential(\n","            nn.ConvTranspose2d(ch, 64, 2, stride=2), Snake(),\n","            nn.Conv2d(64, 32, 3, padding=1), Snake(),\n","            nn.Conv2d(32, 1, 1), nn.Tanh()\n","        )\n","\n","    def reparam(self, mu, logv):\n","        std = (0.5 * logv).exp()\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def forward(self, x):\n","        h  = self.enc(x)\n","        mu = self.mu(h)\n","        lv = self.logvar(h)\n","\n","        # Sample during training\n","        if self.training:\n","            z = self.reparam(mu, lv)\n","        else:\n","            z = mu\n","\n","        xh = self.dec(z)\n","\n","        # KL map\n","        kl_map = -0.5 * (1 + lv - mu.pow(2) - lv.exp())   # (B,C,H,W)\n","        kl_per_sample = kl_map.mean(dim=[1, 2, 3])        # average over latent dims\n","        kld = kl_per_sample.mean()                        # average over batch\n","\n","        return xh, {\"kld\": kld}\n","\n","class VectorQuantizer(nn.Module):\n","    def __init__(self, K, D, beta=cfg.COMMIT_BETA):\n","        super().__init__()\n","        self.K, self.D, self.beta = K, D, beta\n","        self.emb = nn.Embedding(K, D)\n","        self.emb.weight.data.uniform_(-1.0 / D, 1.0 / D)\n","    def forward(self, z):\n","        zf   = z.permute(0,2,3,1).contiguous()\n","        flat = zf.view(-1, self.D)\n","        dist = (flat.pow(2).sum(1, keepdim=True)\n","                - 2 * flat @ self.emb.weight.t()\n","                + self.emb.weight.pow(2).sum(1))\n","        ind  = dist.argmin(1)\n","        zq   = self.emb(ind).view_as(zf)\n","        # losses\n","        commit = self.beta * ((zq - zf.detach())**2).mean()\n","        codebk = ((zf - zq.detach())**2).mean()\n","        loss = commit + codebk\n","        # straight-through\n","        zq   = zf + (zq - zf).detach()\n","        return zq.permute(0,3,1,2).contiguous(), loss\n","\n","class VQVE(nn.Module):\n","    \"\"\"Single-level VQ-VAE.\"\"\"\n","    def __init__(self, ch=cfg.BOTTLENECK_CH, K=cfg.CODEBOOK_SIZE, beta=cfg.COMMIT_BETA):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            nn.Conv2d(1, 32, 4, stride=2, padding=1), Snake(),\n","            nn.Conv2d(32, ch, 4, stride=2, padding=1), Snake(),\n","        )\n","        self.vq  = VectorQuantizer(K, ch, beta)\n","        self.dec = nn.Sequential(\n","            nn.ConvTranspose2d(ch, 32, 4, stride=2, padding=1), Snake(),\n","            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1), Snake(),\n","            nn.Tanh()\n","        )\n","    def forward(self, x):\n","        z = self.enc(x)\n","        zq, lvq = self.vq(z)\n","        xh = self.dec(zq)\n","        return xh, {\"vq\": lvq}\n","\n","class VQVA2(nn.Module):\n","    def __init__(self, ch=cfg.BOTTLENECK_CH, top_ch=cfg.TOP_CH,\n","                 K=cfg.CODEBOOK_SIZE, beta=cfg.COMMIT_BETA):\n","        super().__init__()\n","\n","        # ---- Bottom encoder: 7×7 ----\n","        self.enc_b = nn.Sequential(\n","            nn.Conv2d(1, 32, 4, stride=2, padding=1), Snake(),\n","            nn.Conv2d(32, ch, 4, stride=2, padding=1), Snake()\n","        )\n","\n","        # ---- Top encoder: 4×4 ----\n","        self.enc_t = nn.Sequential(\n","            nn.Conv2d(ch, top_ch, 4, stride=2, padding=1), Snake()\n","        )\n","\n","        # ---- Quantizers ----\n","        self.vq_t = VectorQuantizer(K, top_ch, beta)\n","        self.vq_b = VectorQuantizer(K, ch, beta)\n","\n","        # ---- Upsample top quantized: ≈7×7 ----\n","        self.up_t = nn.Sequential(\n","            nn.ConvTranspose2d(top_ch, ch, 4, stride=2, padding=1), Snake(),\n","            nn.Conv2d(ch, ch, 3, padding=1), Snake()\n","        )\n","\n","        # ---- Decoder (zq_b + upsampled top): image ----\n","        self.dec = nn.Sequential(\n","            nn.ConvTranspose2d(ch * 2, 64, 4, stride=2, padding=1), Snake(),\n","            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), Snake(),\n","            nn.Conv2d(32, 1, 1), Snake(),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        zb = self.enc_b(x)     # (B, ch, 7,7)\n","        zt = self.enc_t(zb)    # (B, top_ch, 4,4)\n","\n","        zqt, lt = self.vq_t(zt)\n","        up = self.up_t(zqt)\n","\n","        # Align to bottom spatially\n","        Hb, Wb = zb.shape[-2:]\n","        Hu, Wu = up.shape[-2:]\n","        dh, dw = Hb - Hu, Wb - Wu\n","        if dh > 0 or dw > 0:\n","            up = nn.functional.pad(up, (0, max(0, dw), 0, max(0, dh)))\n","        elif dh < 0 or dw < 0:\n","            up = up[:, :, :Hb, :Wb]\n","\n","        zb_input = zb + up\n","        zqb, lb = self.vq_b(zb_input)\n","\n","        dec_in = torch.cat([zqb, up], dim=1)  # (B, ch*2, 7,7)\n","        xh = self.dec(dec_in)\n","\n","        return xh, {\"vq_top\": lt, \"vq_bottom\": lb}\n","\n","# ---------------- Training helpers ----------------\n","def forward_losses(model, xb, yb, name: str):\n","    xh, extra = model(xb)\n","    rec = huber_recon(xh, yb)\n","    if name == \"VAE\":\n","        loss = rec + cfg.VAE_BETA * extra[\"kld\"]\n","    elif name == \"VQVE\":\n","        loss = rec + cfg.VQ_WEIGHT * extra[\"vq\"]\n","    elif name == \"VQVA2\":\n","        loss = rec + cfg.VQ_TOP_WEIGHT * extra[\"vq_top\"] + cfg.VQ_BOTTOM_WEIGHT * extra[\"vq_bottom\"]\n","    else:\n","        loss = rec\n","    return xh, loss, rec, extra\n","\n","@torch.no_grad()\n","def test_pass(model, loader, name: str):\n","    model.eval()\n","    r1s, r2s, recs, tots = [], [], [], []\n","    for xb, yb in loader:\n","        xb, yb = xb.to(device), yb.to(device)\n","        if cfg.PRECISION == \"fp64\":\n","            xb = xb.to(torch.float64); yb = yb.to(torch.float64)\n","        xh, loss, rec, _ = forward_losses(model, xb, yb, name)\n","        r1, r2 = rel_errors(xh, yb)\n","        r1s.append(r1.cpu()); r2s.append(r2.cpu())\n","        recs.append(rec.item()); tots.append(loss.item())\n","    r1m = torch.cat(r1s).mean().item()\n","    r2m = torch.cat(r2s).mean().item()\n","    return r1m, r2m, float(np.mean(recs)), float(np.mean(tots)), torch.cat(r1s).numpy(), torch.cat(r2s).numpy()\n","\n","def make_optimizer(model):\n","    name = cfg.OPTIMIZER.lower()\n","    if name == \"soap\":\n","        assert SOAP is not None, \"soap.py not found.\"\n","        return SOAP(\n","            params=model.parameters(),\n","            lr=cfg.SOAP_LR,\n","            betas=cfg.SOAP_BETAS,\n","            weight_decay=cfg.SOAP_WD,\n","            precondition_frequency=cfg.SOAP_PREFREQ\n","        )\n","    # First-order Adam\n","    return torch.optim.Adam(\n","        model.parameters(),\n","        lr=cfg.LR,\n","        betas=cfg.ADAM_BETAS,\n","        weight_decay=cfg.WEIGHT_DECAY\n","    )\n","\n","def run_one(model_class, model_name, tag, tf, trn, vn, tsn):\n","    run_dir = OUT_DIR / f\"{model_name}_{tag}_tr{int(tf*100)}_tn{trn}_vn{vn}_ts{tsn}_{cfg.PRECISION}_{cfg.OPTIMIZER}_{time.strftime('%Y%m%d_%H%M%S')}\"\n","    run_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Data\n","    tr_ds, va_ds, te_ds, trL, vaL, teL = build_loaders(\n","        train_fraction=tf, batch_size=cfg.BATCH_SIZE,\n","        train_noise_pct=trn, val_noise_pct=vn, test_noise_pct=tsn\n","    )\n","\n","    # Model & optim\n","    model = model_class().to(device)\n","    if cfg.PRECISION == \"fp64\":\n","        model = model.double()\n","\n","    opt = make_optimizer(model)\n","    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.EPOCHS)\n","\n","    best_val, no_imp = float(\"inf\"), 0\n","    history = {\"train\": [], \"val\": []}\n","\n","    for ep in range(1, cfg.EPOCHS + 1):\n","        model.train(); t_losses = []\n","        for xb, yb in trL:\n","            xb, yb = xb.to(device), yb.to(device)\n","            if cfg.PRECISION == \"fp64\":\n","                xb = xb.to(torch.float64); yb = yb.to(torch.float64)\n","            opt.zero_grad(set_to_none=True)\n","            _, loss, _, _ = forward_losses(model, xb, yb, model_name)\n","            loss.backward()\n","            if cfg.GRAD_CLIP:\n","                nn.utils.clip_grad_norm_(model.parameters(), cfg.GRAD_CLIP)\n","            opt.step()\n","            t_losses.append(loss.item())\n","        sch.step()\n","\n","        model.eval(); v_losses = []\n","        with torch.no_grad():\n","            for xb, yb in vaL:\n","                xb, yb = xb.to(device), yb.to(device)\n","                if cfg.PRECISION == \"fp64\":\n","                    xb = xb.to(torch.float64); yb = yb.to(torch.float64)\n","                _, vloss, _, _ = forward_losses(model, xb, yb, model_name)\n","                v_losses.append(vloss.item())\n","        v_mean = float(np.mean(v_losses)) if v_losses else float(\"inf\")\n","        history[\"train\"].append(float(np.mean(t_losses)) if t_losses else float(\"inf\"))\n","        history[\"val\"].append(v_mean)\n","\n","        # central per-epoch\n","        log_loss(\n","            {\"run_name\": run_dir.name, \"run_path\": str(run_dir.resolve()), \"model\": model_name, \"scenario\": tag},\n","            ep, history[\"train\"][-1], v_mean\n","        )\n","\n","        improved = (best_val - v_mean) > cfg.MIN_DELTA\n","        if improved:\n","            best_val, no_imp = v_mean, 0\n","        else:\n","            no_imp += 1\n","\n","        print(f\"[{model_name}/{tag}/{cfg.PRECISION}/{cfg.OPTIMIZER}] epoch {ep:03d} | val={v_mean:.6f}\")\n","\n","        if (cfg.PATIENCE is not None) and (no_imp >= cfg.PATIENCE):\n","            print(\"Early stopping\")\n","            break\n","\n","    # Save local history files\n","    with (run_dir / \"history.json\").open(\"w\") as f:\n","        json.dump(history, f, indent=2)\n","    with (run_dir / \"loss_history.csv\").open(\"w\", newline=\"\") as f:\n","        w = csv.writer(f); w.writerow([\"epoch\",\"train_total\",\"val_total\"])\n","        for e,(tr,va) in enumerate(zip(history[\"train\"], history[\"val\"]), start=1):\n","            w.writerow([e, tr, va])\n","\n","    # Test\n","    r1m, r2m, rec_m, tot_m, r1_arr, r2_arr = test_pass(model, teL, model_name)\n","    np.savez_compressed(run_dir / \"per_sample_metrics.npz\",\n","                        relL1=r1_arr, relL2=r2_arr, indices=np.arange(r1_arr.shape[0]))\n","\n","    # Report Rel L1 / L2 to stdout\n","    print(\n","        f\"[TEST {model_name}/{tag}/{cfg.PRECISION}/{cfg.OPTIMIZER}] \"\n","        f\"recon={rec_m:.6f} total={tot_m:.6f} relL1={r1m:.6f} relL2={r2m:.6f}\"\n","    )\n","\n","    # Save checkpoint for this run\n","    ckpt_path = run_dir / \"model.pt\"\n","    torch.save(\n","        {\n","            \"model_state\": model.state_dict(),\n","            \"model_name\": model_name,\n","            \"cfg\": cfg.__dict__,\n","            \"run_dir\": str(run_dir.resolve()),\n","            \"scenario\": tag,\n","            \"precision\": cfg.PRECISION,\n","            \"optimizer\": cfg.OPTIMIZER,\n","        },\n","        ckpt_path,\n","    )\n","\n","    # central experiment info + metrics\n","    exp_row = {\n","        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n","        \"dataset\": \"inet100\",\n","        \"run_name\": run_dir.name, \"run_path\": str(run_dir.resolve()),\n","        \"model\": model_name, \"scenario\": tag, \"sc\": tag[:1],\n","        \"epochs\": cfg.EPOCHS, \"batch_size\": cfg.BATCH_SIZE, \"seed\": cfg.SEED,\n","        \"train_frac\": tf, \"val_frac\": 1.0 - tf,\n","        \"train_noise_pct\": trn, \"val_noise_pct\": vn, \"test_noise_pct\": tsn,\n","        \"bottleneck_ch\": cfg.BOTTLENECK_CH, \"best_val\": best_val,\n","        \"precision\": cfg.PRECISION, \"optimizer\": cfg.OPTIMIZER\n","    }\n","    log_exp(exp_row)\n","\n","    met_row = {\n","        \"dataset\": \"inet100\",\n","        \"run_name\": run_dir.name, \"run_path\": str(run_dir.resolve()),\n","        \"model\": model_name, \"scenario\": tag, \"sc\": tag[:1],\n","        \"train_frac\": tf, \"val_frac\": 1.0 - tf,\n","        \"train_noise_pct\": trn, \"val_noise_pct\": vn, \"test_noise_pct\": tsn,\n","        \"recon_huber_mean\": rec_m, \"aux_loss_mean\": 0.0, \"total_loss_mean\": tot_m,\n","        \"relL1_mean\": r1m, \"relL2_mean\": r2m,\n","        \"train_size\": len(tr_ds), \"val_size\": len(va_ds), \"test_size\": len(te_ds),\n","        \"precision\": cfg.PRECISION, \"optimizer\": cfg.OPTIMIZER\n","    }\n","    log_metrics(met_row)\n","\n","    print(f\"[DONE {model_name}/{tag}/{cfg.PRECISION}/{cfg.OPTIMIZER}] {run_dir}\")\n","\n","    # Return info for global best selection\n","    return {\n","        \"run_dir\": str(run_dir.resolve()),\n","        \"model_name\": model_name,\n","        \"scenario\": tag,\n","        \"train_frac\": tf,\n","        \"train_noise_pct\": trn,\n","        \"val_noise_pct\": vn,\n","        \"test_noise_pct\": tsn,\n","        \"precision\": cfg.PRECISION,\n","        \"optimizer\": cfg.OPTIMIZER,\n","        \"total_loss_mean\": tot_m,\n","        \"ckpt_path\": str(ckpt_path),\n","    }\n","\n","def train_all_for_model(model_class, model_name):\n","    results = []\n","    # b) split sweep with clean test\n","    for tf in cfg.TRAIN_FRACTIONS_B:\n","        results.append(run_one(model_class, model_name, \"b_split\", tf, 0.0, 0.0, 0.0))\n","    # c) fixed split with test noise sweep\n","    for tn in cfg.TEST_NOISES:\n","        results.append(run_one(model_class, model_name, \"c_test_noise\", cfg.TRAIN_FRACTION_FIXED, 0.0, 0.0, float(tn)))\n","    # d) noise on train (same pct) + same test noise\n","    for tn in cfg.TEST_NOISES:\n","        results.append(run_one(model_class, model_name, \"d_train_and_test_noise\", cfg.TRAIN_FRACTION_FIXED, float(tn), 0.0, float(tn)))\n","    print(f\"[{model_name}] all scenarios complete.\")\n","    return results\n","\n","# ---------------- Run models ----------------\n","ALL_MODELS = [\n","    # (AuE,   \"AuE\"),\n","    (VAE,   \"VAE\"),\n","    # (VQVE,  \"VQVE\"),\n","    # (VQVA2, \"VQVA2\"),\n","]\n","\n","# Collect all run results for global best selection\n","all_results_by_model = {name: [] for (_, name) in ALL_MODELS}\n","\n","# 4-case sweep: (fp32|fp64) × (adam|soap)\n","for prec in [\"fp32\", \"fp64\"]:\n","    cfg.PRECISION = prec\n","    torch.set_default_dtype(torch.float64 if cfg.PRECISION == \"fp64\" else torch.float32)\n","    for opt in [\"adam\", \"soap\"]:\n","        cfg.OPTIMIZER = opt\n","        if cfg.OPTIMIZER == \"soap\" and SOAP is None:\n","            raise RuntimeError(\"Requested SOAP but soap.py not found. Place soap.py next to this script.\")\n","        for cls, name in ALL_MODELS:\n","            results = train_all_for_model(cls, name)\n","            all_results_by_model[name].extend(results)\n","\n","# Global best per model (over all scenarios / prec / optimizers)\n","for model_name, runs in all_results_by_model.items():\n","    if not runs:\n","        continue\n","    best_run = min(runs, key=lambda r: r[\"total_loss_mean\"])\n","    ckpt_path = best_run[\"ckpt_path\"]\n","    state = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n","    best_out_path = OUT_DIR / f\"best_overall_{model_name}.pt\"\n","    torch.save(state, best_out_path)\n","    print(\n","        f\"\\n[GLOBAL BEST] {model_name} total_loss_mean={best_run['total_loss_mean']:.6f} \"\n","        f\"(scenario={best_run['scenario']}, prec={best_run['precision']}, opt={best_run['optimizer']})\\n\"\n","        f\"Saved to: {best_out_path}\"\n","    )\n","\n","print(\"\\nAll models completed. Central logs:\")\n","print(\"  -\", cfg.EXP_CSV.resolve())\n","print(\"  -\", cfg.LOSS_CSV.resolve())\n","print(\"  -\", cfg.MET_CSV.resolve())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZ2OkFc2Xkla"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
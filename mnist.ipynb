{"cells":[{"cell_type":"markdown","metadata":{"id":"ZJj8fj8KziJ4"},"source":["#### AutoEncoders Implementation\n","#### AE-VAE-VQVAE-VQVAE2\n","#### Dataset: MNIST"]},{"cell_type":"code","source":["# Connect Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/MyDrive/Colab Notebooks/Colab Notebooks/autoencoders\n","\n","%pwd"],"metadata":{"id":"Xz0FNqTjKgCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, csv, json, time, gzip, struct\n","from datetime import datetime\n","from pathlib import Path\n","\n","import numpy as np\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from torchvision import transforms\n","\n","# SOAP import (second-order optimizer)\n","try:\n","    from soap import SOAP\n","except Exception:\n","    SOAP = None\n","\n","\n","class Cfg:\n","    ROOT      = Path(\".\")\n","    #ROOT      = Path(\"/home/krajnish/autoencoders\")\n","    DATA_DIR  = ROOT / \"datasets/mnist\"\n","    TRAIN_DIR = DATA_DIR / \"train\"\n","    TEST_DIR  = DATA_DIR / \"test\"\n","    OUT_DIR   = ROOT / \"best_models/output_mnist/output_mnist_vae\"\n","\n","    # Training\n","    EPOCHS       = 1    #300\n","    BATCH_SIZE   = 256\n","    LR           = 1e-3\n","    WEIGHT_DECAY = 1e-4\n","    NUM_WORKERS  = 0\n","    PATIENCE     = None\n","    MIN_DELTA    = 1e-5\n","    GRAD_CLIP    = 1.0\n","\n","    # Optimizer/Precision\n","    PRECISION    = \"fp32\"          # precision: \"fp32\", \"fp64\"\n","    OPTIMIZER    = \"adam\"          # optimizer: \"adam\", \"soap\"\n","    ADAM_BETAS   = (0.9, 0.999)\n","\n","    # SOAP defaults (from official repo examples)\n","    SOAP_LR      = 3e-3\n","    SOAP_BETAS   = (0.95, 0.95)\n","    SOAP_WD      = 1e-2\n","    SOAP_PREFREQ = 10\n","\n","    # Shared latent/channel sizes\n","    BOTTLENECK_CH = 56\n","    LATENT_SHAPE  = (56, 7, 7)\n","\n","    # VQ specifics\n","    CODEBOOK_SIZE = 512\n","    COMMIT_BETA   = 0.25\n","\n","    # VQ loss weights\n","    VQ_WEIGHT         = 1.0   # for VQVAE\n","    VQ_TOP_WEIGHT     = 1.0   # for VQVA2 top level\n","    VQ_BOTTOM_WEIGHT  = 1.0   # for VQVA2 bottom level\n","\n","    # VQ-VAE-2 specifics\n","    TOP_CH        = 56\n","\n","    # VAE KL weight\n","    VAE_BETA      = 1.0\n","\n","    # Scenarios\n","    #TRAIN_FRACTIONS_B = [0.50, 0.60, 0.70]     # scenario b\n","    TRAIN_FRACTIONS_B = [0.70]\n","    #TEST_NOISES       = [1, 5, 10]             # scenarios c & d\n","    TEST_NOISES       = [1]\n","    TRAIN_FRACTION_FIXED = 0.70                # for c/d\n","\n","    # Checkpoint subdir\n","    CKPT_SUBDIR = \"ckpts\"\n","\n","    # Logging\n","    EXP_CSV = OUT_DIR / \"experiments_all.csv\"\n","    LOSS_CSV = OUT_DIR / \"loss_history_all.csv\"\n","    METRICS_CSV = OUT_DIR / \"metrics_all.csv\"\n","\n","    SEED = 42\n","    SHUFFLE = False\n","\n","\n","cfg = Cfg()\n","cfg.OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# Torch device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(cfg.SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(cfg.SEED)\n","\n","print(\"Device:\", device)\n","\n","# Data (IDX loader & noise)\n","to_tensor_norm = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","\n","def _open_idx(path: Path):\n","    return gzip.open(path, \"rb\") if path.suffix == \".gz\" else open(path, \"rb\")\n","\n","\n","def parse_idx_images(path: Path):\n","    with _open_idx(path) as f:\n","        header = f.read(16)\n","        if len(header) != 16:\n","            raise RuntimeError(\"Malformed IDX header\")\n","        magic, num, rows, cols = struct.unpack(\">IIII\", header)\n","        if magic != 2051:\n","            raise RuntimeError(f\"Bad magic {magic}\")\n","        data = np.frombuffer(f.read(), dtype=np.uint8)\n","    return data.reshape(num, rows, cols)\n","\n","\n","def find_idx_in_dir(dir_path: Path, candidates):\n","    for fn in candidates:\n","        p = dir_path / fn\n","        if p.exists():\n","            return p\n","    for p in dir_path.rglob(\"*\"):\n","        if p.is_file() and p.name in candidates:\n","            return p\n","    return None\n","\n","\n","def add_noise(x: torch.Tensor, pct: float) -> torch.Tensor:\n","    if pct <= 0:\n","        return x\n","    std = float(pct) / 100.0\n","    return (x + torch.randn_like(x) * std).clamp(-1.0, 1.0)\n","\n","\n","class MNISTIdxDataset(Dataset):\n","    def __init__(self, images: np.ndarray, transform=None, noise_pct: float = 0.0):\n","        self.images = images\n","        self.transform = transform\n","        self.noise_pct = float(noise_pct)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img = Image.fromarray(self.images[idx], mode=\"L\")\n","        x_clean = self.transform(img) if self.transform else transforms.ToTensor()(img)\n","        x_noisy = add_noise(x_clean, self.noise_pct)\n","        return x_noisy, x_clean\n","\n","\n","# Load IDX paths\n","train_idx = find_idx_in_dir(cfg.TRAIN_DIR, [\"train-images.idx3-ubyte\", \"train-images-idx3-ubyte.gz\"])\n","test_idx  = find_idx_in_dir(cfg.TEST_DIR,  [\"t10k-images.idx3-ubyte\", \"t10k-images-idx3-ubyte.gz\"])\n","if train_idx is None or test_idx is None:\n","    raise RuntimeError(\"Expect 60k train + 10k test IDX in mnist/{train,test}\")\n","\n","TRAIN_FULL = parse_idx_images(train_idx)\n","TEST_FULL  = parse_idx_images(test_idx)\n","\n","\n","def build_loaders(train_fraction: float,\n","                  batch_size: int,\n","                  train_noise_pct: float = 0.0,\n","                  val_noise_pct: float   = 0.0,\n","                  test_noise_pct: float  = 0.0):\n","    n_full = len(TRAIN_FULL)\n","    n_train = int(train_fraction * n_full)\n","    train_ds = MNISTIdxDataset(TRAIN_FULL[:n_train], transform=to_tensor_norm, noise_pct=train_noise_pct)\n","    val_ds   = MNISTIdxDataset(TRAIN_FULL[n_train:], transform=to_tensor_norm, noise_pct=val_noise_pct)\n","    test_ds  = MNISTIdxDataset(TEST_FULL,            transform=to_tensor_norm, noise_pct=test_noise_pct)\n","    pin = (device.type == \"cuda\")\n","    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=cfg.SHUFFLE,\n","                              num_workers=cfg.NUM_WORKERS, pin_memory=pin)\n","    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=cfg.SHUFFLE,\n","                              num_workers=cfg.NUM_WORKERS, pin_memory=pin)\n","    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=cfg.SHUFFLE,\n","                              num_workers=cfg.NUM_WORKERS, pin_memory=pin)\n","    return train_ds, val_ds, test_ds, train_loader, val_loader, test_loader\n","\n","\n","# ---------------- Snake activation ----------------\n","class Snake(nn.Module):\n","    \"\"\"\n","    Snake activation: x + (1/a) * sin^2(a x)\n","    \"\"\"\n","    def __init__(self, alpha=1.0):\n","        super().__init__()\n","        self.alpha = nn.Parameter(torch.tensor(alpha))\n","\n","    def forward(self, x):\n","        a = self.alpha.abs() + 1e-6\n","        return x + (1.0 / a) * torch.sin(a * x).pow(2)\n","\n","\n","# Models\n","class EncoderBlock(nn.Module):\n","    def __init__(self, cin, cout):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(cin, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","            nn.Conv2d(cout, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, cin, cout):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(cin, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","            nn.Conv2d(cout, cout, 3, padding=1),\n","            nn.GroupNorm(num_groups=min(8, cout), num_channels=cout), Snake(),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","# --- AuE (plain autoencoder) ---\n","class AuE(nn.Module):\n","    \"\"\"Autoencoder with latent 56x7x7.\"\"\"\n","    def __init__(self, ch=56):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 28->14\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, ch, 4, stride=2, padding=1),  # 14->7 -> latent (ch,7,7)\n","            Snake(),\n","        )\n","        self.dec = nn.Sequential(\n","            DecoderBlock(ch, 128),\n","            nn.ConvTranspose2d(128, 64, 2, stride=2),   # 7->14\n","            Snake(),\n","            DecoderBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2),    # 14->28\n","            Snake(),\n","            nn.Conv2d(32, 1, 1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        z = self.enc(x)\n","        xhat = self.dec(z)\n","        return xhat, {\"aux_loss\": torch.tensor(0.0, device=x.device)}\n","\n","\n","# --- VAE ---\n","class VAE(nn.Module):\n","    \"\"\"VAE with latent 56x7x7 (Gaussian per latent cell).\"\"\"\n","    def __init__(self, ch=56, beta_kl=1.0):\n","        super().__init__()\n","        self.beta_kl = beta_kl\n","\n","        self.enc = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # → (128,7,7)\n","            Snake(),\n","        )\n","        self.mu     = nn.Conv2d(128, ch, 1)\n","        self.logvar = nn.Conv2d(128, ch, 1)\n","        self.dec = nn.Sequential(\n","            DecoderBlock(ch, 128),\n","            nn.ConvTranspose2d(128, 64, 2, stride=2),\n","            Snake(),\n","            DecoderBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2),\n","            Snake(),\n","            nn.Conv2d(32, 1, 1),\n","            nn.Tanh(),\n","        )\n","\n","    def reparam(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def forward(self, x):\n","        h = self.enc(x)\n","        mu, logvar = self.mu(h), self.logvar(h)\n","\n","        if self.training:\n","            z = self.reparam(mu, logvar)\n","        else:\n","            z = mu\n","\n","        xhat = self.dec(z)\n","\n","        # KL per-sample\n","        kl_map = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())   # (B,C,H,W)\n","        kl_per_sample = kl_map.mean(dim=[1, 2, 3])                # avg over C,H,W\n","        kl = kl_per_sample.mean()                                 # avg over batch\n","\n","        kl_weighted = self.beta_kl * kl\n","\n","        return xhat, {\n","            \"aux_loss\": kl_weighted,\n","            \"kl\": kl,\n","            \"mu\": mu,\n","            \"logvar\": logvar,\n","        }\n","\n","\n","# --- Vector Quantizer ---\n","class VectorQuantizer(nn.Module):\n","    def __init__(self, K, D, beta_commit=0.25):\n","        super().__init__()\n","        self.K = K\n","        self.D = D\n","        self.beta = beta_commit\n","        self.codebook = nn.Embedding(K, D)\n","        # Use D for initialization range\n","        nn.init.uniform_(self.codebook.weight, -1.0 / D, 1.0 / D)\n","\n","    def forward(self, z_e):\n","        B, D, H, W = z_e.shape\n","        z = z_e.permute(0, 2, 3, 1).contiguous().view(-1, D)  # (BHW, D)\n","        e = self.codebook.weight\n","        dist = (z.pow(2).sum(1, keepdim=True) + e.pow(2).sum(1) - 2 * z @ e.t())\n","        idx = torch.argmin(dist, dim=1)\n","        z_q = self.codebook(idx).view(B, H, W, D).permute(0, 3, 1, 2).contiguous()\n","\n","        codebook_loss = F.mse_loss(z_q.detach(), z_e)\n","        commit_loss   = self.beta * F.mse_loss(z_q, z_e.detach())\n","        vq_loss = codebook_loss + commit_loss\n","\n","        z_q_st = z_e + (z_q - z_e).detach()\n","\n","        with torch.no_grad():\n","            one_hot = F.one_hot(idx, num_classes=self.K).float()\n","            avg_probs = one_hot.mean(0)\n","            perplexity = torch.exp(-(avg_probs * (avg_probs + 1e-10).log()).sum())\n","        return z_q_st, vq_loss, perplexity, idx.view(B, H, W)\n","\n","\n","# --- VQVAE ---\n","class VQVAE(nn.Module):\n","    \"\"\"VQ-VAE with latent 56x7x7 (codebook dim = 56).\"\"\"\n","    def __init__(self, K=512, D=56, beta_commit=0.25):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 28→14\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, D, 4, stride=2, padding=1),   # 14→7 → (D,7,7)\n","            Snake(),  # activation after encoder\n","        )\n","        self.quant = VectorQuantizer(K, D, beta_commit=beta_commit)\n","        self.decoder = nn.Sequential(\n","            DecoderBlock(D, 128),\n","            nn.ConvTranspose2d(128, 64, 2, stride=2),   # 7→14\n","            Snake(),\n","            DecoderBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2),    # 14→28\n","            Snake(),\n","            nn.Conv2d(32, 1, 1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        z_e = self.encoder(x)\n","        z_q, vq_loss, ppl, idx = self.quant(z_e)\n","        xhat = self.decoder(z_q)\n","        aux = {\n","            \"aux_loss\": vq_loss,\n","            \"vq_loss\": vq_loss,\n","            \"perplexity\": ppl,\n","            \"indices\": idx,\n","        }\n","        return xhat, aux\n","\n","\n","# --- VQVA2 (VQ-VAE-2; two scales) ---\n","class VQVA2(nn.Module):\n","    def __init__(self, K=512, D=56, beta_commit=0.25, top_ch=56):\n","        super().__init__()\n","        self.enc_bottom = nn.Sequential(\n","            EncoderBlock(1, 32),\n","            nn.Conv2d(32, 64, 4, stride=2, padding=1),   # 28→14\n","            Snake(),\n","            EncoderBlock(64, 64),\n","            nn.Conv2d(64, D, 4, stride=2, padding=1),    # 14→7 → (D,7,7)\n","            Snake(),  # activation after bottom encoder\n","        )\n","        self.enc_top = nn.Sequential(\n","            nn.Conv2d(D, 128, 3, padding=1),\n","            Snake(),\n","            nn.Conv2d(128, top_ch, 4, stride=2, padding=1),  # 7→4\n","            Snake(),  # activation after top encoder\n","        )\n","        self.quant_top = VectorQuantizer(K, top_ch, beta_commit=beta_commit)\n","        self.bottom_condition = nn.Sequential(\n","            nn.Conv2d(D + top_ch, D, 1),\n","            Snake()\n","        )\n","        self.quant_bottom = VectorQuantizer(K, D, beta_commit=beta_commit)\n","        self.dec = nn.Sequential(\n","            DecoderBlock(D + top_ch, 128),\n","            nn.ConvTranspose2d(128, 64, 2, stride=2),   # 7→14\n","            Snake(),\n","            DecoderBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, 2, stride=2),    # 14→28\n","            Snake(),\n","            nn.Conv2d(32, 1, 1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        zb_e = self.enc_bottom(x)                       # (D,7,7)\n","        zt_e = self.enc_top(zb_e)                       # (top_ch,4,4)\n","        zt_q, vq_t, ppl_t, idx_t = self.quant_top(zt_e)\n","        zt_up = F.interpolate(zt_q, size=zb_e.shape[-2:], mode=\"nearest\")  # (top_ch,7,7)\n","        zb_cond = self.bottom_condition(torch.cat([zb_e, zt_up], dim=1))   # (D,7,7)\n","        zb_q, vq_b, ppl_b, idx_b = self.quant_bottom(zb_cond)\n","        dec_in = torch.cat([zb_q, zt_up], dim=1)        # (D+top_ch,7,7)\n","        xhat = self.dec(dec_in)\n","        aux = {\n","            \"aux_loss\": cfg.VQ_TOP_WEIGHT * vq_t + cfg.VQ_BOTTOM_WEIGHT * vq_b,\n","            \"vq_top\": vq_t,\n","            \"vq_bottom\": vq_b,\n","            \"perplexity_top\": ppl_t,\n","            \"perplexity_bottom\": ppl_b,\n","            \"indices_top\": idx_t,\n","            \"indices_bottom\": idx_b\n","        }\n","        return xhat, aux\n","\n","\n","# Losses & metrics\n","def huber_recon(xhat, y, delta=1.0):\n","    diff = xhat - y\n","    absd = diff.abs()\n","    quad = torch.clamp(absd, max=delta)\n","    lin  = absd - quad\n","    return (0.5 * quad * quad / delta + lin).mean()\n","\n","\n","def rel_errors(xhat, y, eps=1e-12):\n","    diff = xhat - y\n","    relL1 = diff.abs().flatten(1).sum(1) / (y.abs().flatten(1).sum(1) + eps)\n","    relL2 = torch.sqrt((diff**2).flatten(1).sum(1)) / (torch.sqrt((y**2).flatten(1).sum(1) + eps))\n","    return relL1, relL2\n","\n","\n","# Logging utilities (single CSVs)\n","def _ensure_header(path: Path, fieldnames):\n","    write_header = not path.exists()\n","    f = path.open(\"a\", newline=\"\")\n","    w = csv.DictWriter(f, fieldnames=fieldnames)\n","    if write_header:\n","        w.writeheader()\n","    return f, w\n","\n","\n","def log_experiment_row(row: dict):\n","    fields = [\n","        \"timestamp\",\"run_name\",\"run_path\",\"model\",\"scenario\",\"sc\",\n","        \"epochs\",\"batch_size\",\"seed\",\n","        \"train_frac\",\"val_frac\",\"train_noise_pct\",\"val_noise_pct\",\"test_noise_pct\",\n","        \"bottleneck_ch\",\"codebook_size\",\"commitment_beta\",\"top_ch\",\n","        \"precision\",\"optimizer\",\n","        \"best_val\",\"best_ckpt\",\"last_ckpt\"\n","    ]\n","    f, w = _ensure_header(cfg.EXP_CSV, fields)\n","    w.writerow({k: row.get(k, None) for k in fields})\n","    f.close()\n","\n","\n","def log_loss_history_rows(run_name, run_path, model, scenario, epoch, train_total, val_total, ppl=None, extra=None):\n","    fields = [\"run_name\",\"run_path\",\"model\",\"scenario\",\"epoch\",\"train_total\",\"val_total\",\"ppl\",\"extra_json\"]\n","    f, w = _ensure_header(cfg.LOSS_CSV, fields)\n","    w.writerow({\n","        \"run_name\": run_name,\n","        \"run_path\": str(run_path),\n","        \"model\": model,\n","        \"scenario\": scenario,\n","        \"epoch\": epoch,\n","        \"train_total\": train_total,\n","        \"val_total\": val_total,\n","        \"ppl\": ppl if (ppl is not None) else \"\",\n","        \"extra_json\": json.dumps(extra or {})\n","    })\n","    f.close()\n","\n","\n","def log_metrics_row(row: dict):\n","    fields = [\n","        \"run_name\",\"run_path\",\"model\",\"scenario\",\"sc\",\n","        \"train_frac\",\"val_frac\",\"train_noise_pct\",\"val_noise_pct\",\"test_noise_pct\",\n","        \"recon_huber_mean\",\"aux_loss_mean\",\"total_loss_mean\",\n","        \"relL1_mean\",\"relL2_mean\",\n","        \"train_size\",\"val_size\",\"test_size\",\n","        \"precision\",\"optimizer\"\n","    ]\n","    f, w = _ensure_header(cfg.METRICS_CSV, fields)\n","    w.writerow({k: row.get(k, None) for k in fields})\n","    f.close()\n","\n","\n","# Train/Eval\n","class CheckpointManager:\n","    def __init__(self, ckpt_dir: Path):\n","        self.ckpt_dir = Path(ckpt_dir)\n","        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n","        self.best_path = self.ckpt_dir / \"best.pt\"\n","        self.last_path = self.ckpt_dir / \"last.pt\"\n","\n","    def save(self, epoch, model, optimizer, scheduler, best_val, val_loss, history, extras=None):\n","        state = {\n","            \"epoch\": epoch,\n","            \"model\": model.state_dict(),\n","            \"optimizer\": optimizer.state_dict() if optimizer else None,\n","            \"scheduler\": scheduler.state_dict() if scheduler else None,\n","            \"best_val\": best_val,\n","            \"val_loss\": val_loss,\n","            \"history\": history,\n","            \"extras\": extras or {},\n","            \"seed\": cfg.SEED,\n","        }\n","        torch.save(state, self.last_path)\n","        if val_loss + 1e-12 < best_val - cfg.MIN_DELTA:\n","            torch.save(state, self.best_path)\n","            return True\n","        return False\n","\n","\n","def build_model(model_name: str):\n","    if model_name == \"AuE\":\n","        return AuE(ch=cfg.BOTTLENECK_CH)\n","    if model_name == \"VAE\":\n","        return VAE(ch=cfg.BOTTLENECK_CH, beta_kl=cfg.VAE_BETA)\n","    if model_name == \"VQVAE\":\n","        return VQVAE(K=cfg.CODEBOOK_SIZE, D=cfg.BOTTLENECK_CH, beta_commit=cfg.COMMIT_BETA)\n","    if model_name == \"VQVA2\":\n","        return VQVA2(K=cfg.CODEBOOK_SIZE, D=cfg.BOTTLENECK_CH, beta_commit=cfg.COMMIT_BETA, top_ch=cfg.TOP_CH)\n","    raise ValueError(f\"Unknown model: {model_name}\")\n","\n","\n","def forward_and_losses(model_name: str, model, xb, yb):\n","    xhat, info = model(xb)\n","    rec = huber_recon(xhat, yb)\n","    aux_term = info.get(\"aux_loss\", torch.tensor(0.0, device=xb.device, dtype=xb.dtype))\n","\n","    if model_name == \"VAE\":\n","        loss = rec + aux_term\n","        ppl = None\n","\n","    elif model_name == \"VQVAE\":\n","        vq_loss = info.get(\"vq_loss\", aux_term)\n","        loss = rec + cfg.VQ_WEIGHT * vq_loss\n","        ppl = float(info.get(\"perplexity\", float(\"nan\")))\n","\n","    elif model_name == \"VQVA2\":\n","        vq_t = info.get(\"vq_top\", None)\n","        vq_b = info.get(\"vq_bottom\", None)\n","        if (vq_t is None) or (vq_b is None):\n","            loss = rec + aux_term\n","        else:\n","            loss = rec + cfg.VQ_TOP_WEIGHT * vq_t + cfg.VQ_BOTTOM_WEIGHT * vq_b\n","        ppl_t = float(info.get(\"perplexity_top\", float(\"nan\")))\n","        ppl_b = float(info.get(\"perplexity_bottom\", float(\"nan\")))\n","        ppl = (ppl_t + ppl_b) / 2.0\n","\n","    else:\n","        loss = rec\n","        ppl = None\n","\n","    return xhat, loss, rec, aux_term, ppl\n","\n","\n","def make_optimizer(model):\n","    opt_name = cfg.OPTIMIZER.lower()\n","    if opt_name == \"soap\":\n","        assert SOAP is not None, \"SOAP optimizer not found\"\n","        return SOAP(\n","            params=model.parameters(),\n","            lr=cfg.SOAP_LR,\n","            betas=cfg.SOAP_BETAS,\n","            weight_decay=cfg.SOAP_WD,\n","            precondition_frequency=cfg.SOAP_PREFREQ\n","        )\n","    # default: Adam (first-order)\n","    return torch.optim.Adam(\n","        model.parameters(),\n","        lr=cfg.LR,\n","        betas=cfg.ADAM_BETAS,\n","        weight_decay=cfg.WEIGHT_DECAY\n","    )\n","\n","\n","def train_eval_once(model_name: str,\n","                    train_frac: float,\n","                    train_noise_pct: float,\n","                    val_noise_pct: float,\n","                    test_noise_pct: float,\n","                    tag: str):\n","\n","    run_root = cfg.OUT_DIR / f\"{model_name.lower()}_{tag}_{int(train_frac*100)}_tn{train_noise_pct}_vn{val_noise_pct}_ts{test_noise_pct}_{cfg.PRECISION}_{cfg.OPTIMIZER}_{time.strftime('%Y%m%d_%H%M%S')}\"\n","    run_root.mkdir(parents=True, exist_ok=True)\n","    ckpt_dir = run_root / cfg.CKPT_SUBDIR\n","    cpm = CheckpointManager(ckpt_dir)\n","\n","    train_ds, val_ds, test_ds, train_loader, val_loader, test_loader = build_loaders(\n","        train_fraction=train_frac, batch_size=cfg.BATCH_SIZE,\n","        train_noise_pct=train_noise_pct, val_noise_pct=val_noise_pct, test_noise_pct=test_noise_pct\n","    )\n","\n","    model = build_model(model_name).to(device)\n","    if cfg.PRECISION == \"fp64\":\n","        model = model.double()\n","\n","    opt = make_optimizer(model)\n","    sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=max(5, cfg.EPOCHS))\n","\n","    best_val, no_imp = float(\"inf\"), 0\n","    history = {\"train_total\": [], \"val_total\": [], \"ppl\": []}\n","\n","    for epoch in range(1, cfg.EPOCHS + 1):\n","        # ---- Train\n","        model.train()\n","        t_losses, t_ppls = [], []\n","        for xb, yb in train_loader:\n","            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n","            if cfg.PRECISION == \"fp64\":\n","                xb = xb.to(torch.float64)\n","                yb = yb.to(torch.float64)\n","\n","            opt.zero_grad(set_to_none=True)\n","            xhat, loss, rec, aux, ppl = forward_and_losses(model_name, model, xb, yb)\n","            loss.backward()\n","            if cfg.GRAD_CLIP:\n","                nn.utils.clip_grad_norm_(model.parameters(), cfg.GRAD_CLIP)\n","            opt.step()\n","\n","            t_losses.append(loss.item())\n","            if ppl is not None:\n","                t_ppls.append(ppl)\n","        sch.step(epoch)\n","\n","        # ---- Val\n","        model.eval()\n","        v_losses, v_ppls = [], []\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                xb, yb = xb.to(device), yb.to(device)\n","                if cfg.PRECISION == \"fp64\":\n","                    xb = xb.to(torch.float64)\n","                    yb = yb.to(torch.float64)\n","                _, loss, rec, aux, ppl = forward_and_losses(model_name, model, xb, yb)\n","                v_losses.append(loss.item())\n","                if ppl is not None:\n","                    v_ppls.append(ppl)\n","        v_mean = float(np.mean(v_losses)) if v_losses else float(\"inf\")\n","        ppl_mean = float(np.mean(v_ppls)) if v_ppls else float(\"nan\")\n","        history[\"train_total\"].append(float(np.mean(t_losses)) if t_losses else float(\"inf\"))\n","        history[\"val_total\"].append(v_mean)\n","        history[\"ppl\"].append(ppl_mean)\n","\n","        # log per-epoch row to central loss CSV\n","        log_loss_history_rows(\n","            run_name=run_root.name, run_path=str(run_root), model=model_name, scenario=tag,\n","            epoch=epoch, train_total=history[\"train_total\"][-1], val_total=v_mean, ppl=ppl_mean\n","        )\n","\n","        # checkpointing\n","        improved = (best_val - v_mean) > cfg.MIN_DELTA\n","        if improved:\n","            best_val, no_imp = v_mean, 0\n","        else:\n","            no_imp += 1\n","\n","        cpm.save(epoch=epoch, model=model, optimizer=opt, scheduler=sch,\n","                 best_val=best_val, val_loss=v_mean, history=history,\n","                 extras={\"scenario\": tag, \"train_frac\": train_frac})\n","\n","        print(f\"[{model_name}/{tag}/{cfg.PRECISION}/{cfg.OPTIMIZER}] Epoch {epoch:03d} | val={v_mean:.6f} | ppl={ppl_mean if not np.isnan(ppl_mean) else '—'}\")\n","\n","        if (cfg.PATIENCE is not None) and (no_imp >= cfg.PATIENCE):\n","            print(\"Early stopping.\")\n","            break\n","\n","    with open(run_root / \"history.json\", \"w\") as f:\n","        json.dump(history, f, indent=2)\n","    with open(run_root / \"loss_history.csv\", \"w\", newline=\"\") as f:\n","        w = csv.writer(f)\n","        w.writerow([\"epoch\",\"train_total\",\"val_total\",\"ppl\"])\n","        for e,(tr,va,pp) in enumerate(zip(history[\"train_total\"], history[\"val_total\"], history[\"ppl\"]), start=1):\n","            w.writerow([e, tr, va, pp])\n","\n","    best_to_load = (ckpt_dir / \"best.pt\") if (ckpt_dir / \"best.pt\").exists() else (ckpt_dir / \"last.pt\")\n","    print(\"Testing with:\", best_to_load)\n","    state = torch.load(best_to_load, map_location=device, weights_only=False)\n","    model.load_state_dict(state[\"model\"])\n","    model.eval()\n","\n","    recon_vals, aux_vals, total_vals, r1_vals, r2_vals = [], [], [], [], []\n","    with torch.no_grad():\n","        for xb, yb in test_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            if cfg.PRECISION == \"fp64\":\n","                xb = xb.to(torch.float64)\n","                yb = yb.to(torch.float64)\n","            xhat, loss, rec, aux, ppl = forward_and_losses(model_name, model, xb, yb)\n","            recon_vals.append(rec.item())\n","            aux_vals.append(float(aux))\n","            total_vals.append(loss.item())\n","            r1, r2 = rel_errors(xhat, yb)\n","            r1_vals.append(r1.cpu())\n","            r2_vals.append(r2.cpu())\n","\n","    recon_mean = float(np.mean(recon_vals))\n","    aux_mean   = float(np.mean(aux_vals)) if aux_vals else 0.0\n","    total_mean = float(np.mean(total_vals))\n","    relL1_mean = torch.cat(r1_vals).mean().item()\n","    relL2_mean = torch.cat(r2_vals).mean().item()\n","\n","    # Save per-sample errors\n","    np.savez_compressed(run_root / \"per_sample_metrics.npz\",\n","                        relL1=torch.cat(r1_vals).numpy(),\n","                        relL2=torch.cat(r2_vals).numpy(),\n","                        indices=np.arange(len(torch.cat(r1_vals))))\n","\n","    # Report Rel L1 / L2 to stdout\n","    print(\n","        f\"[TEST {model_name}/{tag}/{cfg.PRECISION}/{cfg.OPTIMIZER}] \"\n","        f\"recon={recon_mean:.6f} aux={aux_mean:.6f} total={total_mean:.6f} \"\n","        f\"relL1={relL1_mean:.6f} relL2={relL2_mean:.6f}\"\n","    )\n","\n","    # Log registry and metrics\n","    info_row = {\n","        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n","        \"run_name\": run_root.name,\n","        \"run_path\": str(run_root.resolve()),\n","        \"model\": model_name,\n","        \"scenario\": tag,\n","        \"sc\": tag[:1] if tag else tag,\n","        \"epochs\": cfg.EPOCHS,\n","        \"batch_size\": cfg.BATCH_SIZE,\n","        \"seed\": cfg.SEED,\n","        \"train_frac\": train_frac, \"val_frac\": 1.0 - train_frac,\n","        \"train_noise_pct\": train_noise_pct, \"val_noise_pct\": val_noise_pct, \"test_noise_pct\": test_noise_pct,\n","        \"bottleneck_ch\": cfg.BOTTLENECK_CH,\n","        \"codebook_size\": cfg.CODEBOOK_SIZE, \"commitment_beta\": cfg.COMMIT_BETA,\n","        \"top_ch\": cfg.TOP_CH if model_name == \"VQVA2\" else \"\",\n","        \"precision\": cfg.PRECISION,\n","        \"optimizer\": cfg.OPTIMIZER,\n","        \"best_val\": best_val,\n","        \"best_ckpt\": str((ckpt_dir / \"best.pt\").resolve()) if (ckpt_dir / \"best.pt\").exists() else \"\",\n","        \"last_ckpt\": str((ckpt_dir / \"last.pt\").resolve()) if (ckpt_dir / \"last.pt\").exists() else \"\",\n","    }\n","    log_experiment_row(info_row)\n","\n","    metrics_row = {\n","        \"run_name\": run_root.name, \"run_path\": str(run_root.resolve()),\n","        \"model\": model_name, \"scenario\": tag, \"sc\": tag[:1] if tag else tag,\n","        \"train_frac\": train_frac, \"val_frac\": 1.0-train_frac,\n","        \"train_noise_pct\": train_noise_pct, \"val_noise_pct\": val_noise_pct, \"test_noise_pct\": test_noise_pct,\n","        \"recon_huber_mean\": recon_mean, \"aux_loss_mean\": aux_mean, \"total_loss_mean\": total_mean,\n","        \"relL1_mean\": relL1_mean, \"relL2_mean\": relL2_mean,\n","        \"train_size\": len(train_ds), \"val_size\": len(val_ds), \"test_size\": len(test_ds),\n","        \"precision\": cfg.PRECISION, \"optimizer\": cfg.OPTIMIZER\n","    }\n","    log_metrics_row(metrics_row)\n","\n","    print(f\"[DONE {model_name}/{tag}/{cfg.PRECISION}/{cfg.OPTIMIZER}] run_dir → {run_root.resolve()}\")\n","    return run_root, info_row, metrics_row\n","\n","\n","# Scenario runner\n","def run_scenarios_for_model(model_name: str):\n","    print(f\"\\n==================== {model_name} ({cfg.PRECISION}/{cfg.OPTIMIZER}) ====================\")\n","    results = []\n","\n","    # b) split sweep, clean test\n","    for tf in cfg.TRAIN_FRACTIONS_B:\n","        results.append(train_eval_once(model_name, tf, 0.0, 0.0, 0.0, tag=\"b_split\"))\n","\n","    # c) fixed split, test noise sweep\n","    for tn in cfg.TEST_NOISES:\n","        results.append(train_eval_once(model_name, cfg.TRAIN_FRACTION_FIXED, 0.0, 0.0, float(tn), tag=\"c_test_noise\"))\n","\n","    # d) fixed split, noise only on TRAIN + same test noise\n","    for tn in cfg.TEST_NOISES:\n","        results.append(train_eval_once(model_name, cfg.TRAIN_FRACTION_FIXED, float(tn), 0.0, float(tn), tag=\"d_train_and_test_noise\"))\n","\n","    return results\n","\n","\n","# Run ALL models\n","#ALL_MODELS = [\"AuE\", \"VAE\", \"VQVAE\", \"VQVA2\"]\n","ALL_MODELS = [\"VAE\"]\n","\n","# ---- 4-case sweep: (fp32|fp64) × (adam|soap) ----\n","all_results_by_model = {m: [] for m in ALL_MODELS}\n","\n","for prec in [\"fp32\", \"fp64\"]:\n","    for opt in [\"adam\", \"soap\"]:\n","        cfg.PRECISION = prec\n","        cfg.OPTIMIZER = opt\n","        if cfg.OPTIMIZER == \"soap\" and SOAP is None:\n","            raise RuntimeError(\"Requested SOAP but soap.py not found\")\n","        torch.set_default_dtype(torch.float64 if cfg.PRECISION == \"fp64\" else torch.float32)\n","\n","        for m in ALL_MODELS:\n","            results = run_scenarios_for_model(m)\n","            all_results_by_model[m].extend(results)\n","\n","# After all runs, pick best for each model\n","for m in ALL_MODELS:\n","    runs = all_results_by_model[m]\n","    if not runs:\n","        continue\n","\n","    # Select run with best (lowest) total_loss_mean\n","    best_run = min(runs, key=lambda r: r[2][\"total_loss_mean\"])\n","    best_run_root, best_info, best_metrics = best_run\n","\n","    best_ckpt_path = best_info.get(\"best_ckpt\") or best_info.get(\"last_ckpt\")\n","    if best_ckpt_path:\n","        state = torch.load(best_ckpt_path, map_location=\"cpu\", weights_only=False)\n","        best_out_path = cfg.OUT_DIR / f\"best_overall_{m}.pt\"\n","        torch.save(state, best_out_path)\n","        print(\n","            f\"\\n[GLOBAL BEST] {m} total_loss_mean={best_metrics['total_loss_mean']:.6f} \"\n","            f\"(run={best_info['run_name']}, prec={best_info['precision']}, opt={best_info['optimizer']})\\n\"\n","            f\"Saved to: {best_out_path}\"\n","        )\n","\n","print(\"\\nAll experiments finished.\")\n","print(f\"Registry:   {cfg.EXP_CSV.resolve()}\")\n","print(f\"Loss CSV:   {cfg.LOSS_CSV.resolve()}\")\n","print(f\"Metrics:    {cfg.METRICS_CSV.resolve()}\")"],"metadata":{"id":"uZgjrEItL6NI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2Va4ym9eLv0Q"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}